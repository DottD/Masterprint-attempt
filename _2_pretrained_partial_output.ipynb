{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NfFt3RgHqL9y"
   },
   "source": [
    "Packages dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "2jhiaSqoqIph",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depedencies successfully installed\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U -q git+https://github.com/fchollet/keras.git\n",
    "!pip3 install -U -q git+https://www.github.com/keras-team/keras-contrib.git\n",
    "!pip3 install -U -q git+https://github.com/hadim/keras-toolbox.git\n",
    "!pip3 install -U -q git+https://github.com/DottD/convnets-keras.git\n",
    "!pip3 install -U -q git+https://github.com/python-telegram-bot/python-telegram-bot.git\n",
    "print('Depedencies successfully installed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-1bGbAT4xjoR"
   },
   "source": [
    "Import package dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6460,
     "status": "ok",
     "timestamp": 1528497188446,
     "user": {
      "displayName": "FILIPPO SANTARELLI",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "106244644809659945153"
     },
     "user_tz": -120
    },
    "id": "SPBNVy6SYee8",
    "outputId": "7b8a661f-fa97-4804-c623-8bf60ca3bc16"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, ELU, Dropout, BatchNormalization\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback, ProgbarLogger, TerminateOnNaN, ModelCheckpoint\n",
    "from keras.callbacks import LearningRateScheduler, LambdaCallback, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.utils import HDF5Matrix\n",
    "from keras.applications import ResNet50\n",
    "from convnetskeras2.alexnet import AlexNet \n",
    "from kerastoolbox.callbacks import TelegramMonitor, PrintMonitor\n",
    "from h5datagen import H5DataGen\n",
    "import h5py\n",
    "from io import StringIO\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from tensorboard_logging import Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IAc77emFjJ-P"
   },
   "source": [
    "###Check GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1067,
     "status": "ok",
     "timestamp": 1528497189894,
     "user": {
      "displayName": "FILIPPO SANTARELLI",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "106244644809659945153"
     },
     "user_tz": -120
    },
    "id": "2iElw5nijJgq",
    "outputId": "c1ba0525-5708-4e0f-dc62-e13972497878"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device not found\n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print('GPU device not found')\n",
    "else:\n",
    "  print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "56v557iCYhVK"
   },
   "source": [
    "## Set up parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "k-k3DDo8YqF4"
   },
   "outputs": [],
   "source": [
    "#@title Parameters { run: \"auto\", display-mode: \"form\" }\n",
    "img_shape = 128 #@param {type:\"integer\"}\n",
    "batch_size = 64 #@param {type:\"integer\"}\n",
    "N = 1000 #@param {type:\"integer\"}\n",
    "V = 100 #@param {type:\"integer\"}\n",
    "db_path = \"/Users/MacD/Databases/database.h5\" #@param {type:\"string\"}\n",
    "db_path = os.path.abspath(os.path.normpath(db_path))\n",
    "filename = \"/Users/MacD/Databases/logits.h5\" #@param {type:\"string\"}\n",
    "filename = os.path.abspath(os.path.normpath(filename))\n",
    "architecture = \"alexnet\" #@param [\"alexnet\", \"resnet\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lOU0CFDKc3Zc"
   },
   "source": [
    "## Set up some monitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mNSo9Vw3cxHj"
   },
   "outputs": [],
   "source": [
    "monitors = [\n",
    "  #TelegramMonitor(api_token=\"546794449:AAGzmfH9Oa6277Vsl2T9hRrGnNHHSpEMsd8\", chat_id=\"41795159\", plot_history=1),\n",
    "  PrintMonitor()]\n",
    "def printmsg(*args):\n",
    "  output = StringIO()\n",
    "  print(*args, file=output, end='')\n",
    "  for monitor in monitors:\n",
    "    monitor.notify(message=output.getvalue())\n",
    "  output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fH6eq2n9FfIt"
   },
   "source": [
    "## Save logits from AlexNet\n",
    "Load a pretrained AlexNet (without the last layer), compute the logits from the last used layer and save them to an .h5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "FNo8MsJ7vvYi",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2483632 images belonging to 177 classes.\n",
      "Network created - 4096 logits\n",
      "Output file /Users/MacD/Databases/logits.h5 opened\n",
      "The training dataset will be created\n",
      "Created dataset training (1000 repetitions)\n",
      "Step 1 / 1000 - slice 0:177 DONE in 12.617342233657837 seconds\n",
      "Step 2 / 1000 - slice 177:354 DONE in 11.318016767501831 seconds\n",
      "Step 3 / 1000 - slice 354:531 DONE in 11.682682991027832 seconds\n",
      "Step 4 / 1000 - slice 531:708 DONE in 12.015029907226562 seconds\n",
      "Step 5 / 1000 - slice 708:885 DONE in 12.08167028427124 seconds\n",
      "Step 6 / 1000 - slice 885:1062 DONE in 12.818318843841553 seconds\n",
      "Step 7 / 1000 - slice 1062:1239 DONE in 13.531311988830566 seconds\n",
      "Step 8 / 1000 - slice 1239:1416 DONE in 13.710819005966187 seconds\n",
      "Step 9 / 1000 - slice 1416:1593 DONE in 10.980029106140137 seconds\n",
      "Step 10 / 1000 - slice 1593:1770 DONE in 11.032562732696533 seconds\n",
      "Step 11 / 1000 - slice 1770:1947 DONE in 10.944623947143555 seconds\n",
      "Step 12 / 1000 - slice 1947:2124 DONE in 11.125774145126343 seconds\n",
      "Step 13 / 1000 - slice 2124:2301 DONE in 11.101965188980103 seconds\n",
      "Step 14 / 1000 - slice 2301:2478 DONE in 11.134624004364014 seconds\n",
      "Step 15 / 1000 - slice 2478:2655 DONE in 13.166792154312134 seconds\n",
      "Step 16 / 1000 - slice 2655:2832 DONE in 12.831543207168579 seconds\n",
      "Step 17 / 1000 - slice 2832:3009 DONE in 12.865985870361328 seconds\n",
      "Step 18 / 1000 - slice 3009:3186 DONE in 11.517871856689453 seconds\n",
      "Step 19 / 1000 - slice 3186:3363 DONE in 11.600240230560303 seconds\n",
      "Step 20 / 1000 - slice 3363:3540 DONE in 11.304870128631592 seconds\n",
      "Step 21 / 1000 - slice 3540:3717 DONE in 10.939789056777954 seconds\n",
      "Step 22 / 1000 - slice 3717:3894 DONE in 11.312654972076416 seconds\n",
      "Step 23 / 1000 - slice 3894:4071 DONE in 10.944447994232178 seconds\n",
      "Step 24 / 1000 - slice 4071:4248 DONE in 12.635295867919922 seconds\n",
      "Step 25 / 1000 - slice 4248:4425 DONE in 11.17741584777832 seconds\n",
      "Step 26 / 1000 - slice 4425:4602 DONE in 10.942407131195068 seconds\n",
      "Step 27 / 1000 - slice 4602:4779 DONE in 10.998213052749634 seconds\n",
      "Step 28 / 1000 - slice 4779:4956 DONE in 10.915179967880249 seconds\n",
      "Step 29 / 1000 - slice 4956:5133 DONE in 11.040724992752075 seconds\n",
      "Step 30 / 1000 - slice 5133:5310 DONE in 10.92041015625 seconds\n",
      "Step 31 / 1000 - slice 5310:5487 DONE in 11.037822961807251 seconds\n",
      "Step 32 / 1000 - slice 5487:5664 DONE in 11.040215015411377 seconds\n",
      "Step 33 / 1000 - slice 5664:5841 DONE in 10.97800087928772 seconds\n",
      "Step 34 / 1000 - slice 5841:6018 DONE in 11.017213106155396 seconds\n",
      "Step 35 / 1000 - slice 6018:6195 DONE in 10.957632064819336 seconds\n",
      "Step 36 / 1000 - slice 6195:6372 DONE in 11.166262865066528 seconds\n",
      "Step 37 / 1000 - slice 6372:6549 DONE in 11.093217134475708 seconds\n",
      "Step 38 / 1000 - slice 6549:6726 DONE in 11.219714879989624 seconds\n",
      "Step 39 / 1000 - slice 6726:6903 DONE in 10.889950037002563 seconds\n",
      "Step 40 / 1000 - slice 6903:7080 DONE in 11.016417980194092 seconds\n",
      "Step 41 / 1000 - slice 7080:7257 DONE in 10.909367084503174 seconds\n",
      "Step 42 / 1000 - slice 7257:7434 DONE in 11.09423279762268 seconds\n",
      "Step 43 / 1000 - slice 7434:7611 DONE in 10.915170907974243 seconds\n",
      "Step 44 / 1000 - slice 7611:7788 DONE in 10.955834150314331 seconds\n",
      "Step 45 / 1000 - slice 7788:7965 DONE in 10.976692914962769 seconds\n",
      "Step 46 / 1000 - slice 7965:8142 DONE in 11.032881259918213 seconds\n",
      "Step 47 / 1000 - slice 8142:8319 DONE in 11.089701175689697 seconds\n",
      "Step 48 / 1000 - slice 8319:8496 DONE in 11.030807971954346 seconds\n",
      "Step 49 / 1000 - slice 8496:8673 DONE in 11.187680006027222 seconds\n",
      "Step 50 / 1000 - slice 8673:8850 DONE in 11.167031049728394 seconds\n",
      "Step 51 / 1000 - slice 8850:9027 DONE in 11.000095844268799 seconds\n",
      "Step 52 / 1000 - slice 9027:9204 DONE in 11.453266859054565 seconds\n",
      "Step 53 / 1000 - slice 9204:9381 DONE in 10.97163987159729 seconds\n",
      "Step 54 / 1000 - slice 9381:9558 DONE in 10.863362073898315 seconds\n",
      "Step 55 / 1000 - slice 9558:9735 DONE in 11.062400102615356 seconds\n",
      "Step 56 / 1000 - slice 9735:9912 DONE in 10.959311246871948 seconds\n",
      "Step 57 / 1000 - slice 9912:10089 DONE in 10.99092698097229 seconds\n",
      "Step 58 / 1000 - slice 10089:10266 DONE in 10.912579774856567 seconds\n",
      "Step 59 / 1000 - slice 10266:10443 DONE in 10.939563035964966 seconds\n",
      "Step 60 / 1000 - slice 10443:10620 DONE in 10.946586847305298 seconds\n",
      "Step 61 / 1000 - slice 10620:10797 DONE in 10.930329084396362 seconds\n",
      "Step 62 / 1000 - slice 10797:10974 DONE in 11.233075141906738 seconds\n",
      "Step 63 / 1000 - slice 10974:11151 DONE in 11.091962099075317 seconds\n",
      "Step 64 / 1000 - slice 11151:11328 DONE in 11.347101926803589 seconds\n",
      "Step 65 / 1000 - slice 11328:11505 DONE in 10.995879173278809 seconds\n",
      "Step 66 / 1000 - slice 11505:11682 DONE in 10.992849111557007 seconds\n",
      "Step 67 / 1000 - slice 11682:11859 DONE in 10.908766984939575 seconds\n",
      "Step 68 / 1000 - slice 11859:12036 DONE in 11.001419067382812 seconds\n",
      "Step 69 / 1000 - slice 12036:12213 DONE in 11.044538974761963 seconds\n",
      "Step 70 / 1000 - slice 12213:12390 DONE in 10.973721027374268 seconds\n",
      "Step 71 / 1000 - slice 12390:12567 DONE in 11.140039920806885 seconds\n",
      "Step 72 / 1000 - slice 12567:12744 DONE in 12.527808904647827 seconds\n",
      "Step 73 / 1000 - slice 12744:12921 DONE in 17.604363918304443 seconds\n",
      "Step 74 / 1000 - slice 12921:13098 DONE in 21.007344961166382 seconds\n",
      "Step 75 / 1000 - slice 13098:13275 DONE in 20.403202056884766 seconds\n",
      "Step 76 / 1000 - slice 13275:13452 DONE in 19.959349155426025 seconds\n",
      "Step 77 / 1000 - slice 13452:13629 DONE in 19.187592267990112 seconds\n",
      "Step 78 / 1000 - slice 13629:13806 DONE in 20.035337924957275 seconds\n",
      "Step 79 / 1000 - slice 13806:13983 DONE in 19.14025092124939 seconds\n",
      "Step 80 / 1000 - slice 13983:14160 DONE in 19.24373507499695 seconds\n",
      "Step 81 / 1000 - slice 14160:14337 DONE in 19.453701972961426 seconds\n",
      "Step 82 / 1000 - slice 14337:14514 DONE in 19.339643955230713 seconds\n",
      "Step 83 / 1000 - slice 14514:14691 DONE in 18.226934671401978 seconds\n",
      "Step 84 / 1000 - slice 14691:14868 DONE in 17.826066970825195 seconds\n",
      "Step 85 / 1000 - slice 14868:15045 DONE in 18.2531840801239 seconds\n",
      "Step 86 / 1000 - slice 15045:15222 DONE in 18.226742029190063 seconds\n",
      "Step 87 / 1000 - slice 15222:15399 DONE in 17.834556341171265 seconds\n",
      "Step 88 / 1000 - slice 15399:15576 DONE in 17.84934711456299 seconds\n",
      "Step 89 / 1000 - slice 15576:15753 DONE in 17.079420804977417 seconds\n",
      "Step 90 / 1000 - slice 15753:15930 DONE in 17.53333592414856 seconds\n",
      "Step 91 / 1000 - slice 15930:16107 DONE in 17.9397292137146 seconds\n",
      "Step 92 / 1000 - slice 16107:16284 DONE in 16.729896306991577 seconds\n",
      "Step 93 / 1000 - slice 16284:16461 DONE in 17.291449785232544 seconds\n",
      "Step 94 / 1000 - slice 16461:16638 DONE in 16.159668922424316 seconds\n",
      "Step 95 / 1000 - slice 16638:16815 DONE in 16.321955919265747 seconds\n",
      "Step 96 / 1000 - slice 16815:16992 DONE in 16.541409015655518 seconds\n",
      "Step 97 / 1000 - slice 16992:17169 DONE in 16.219842672348022 seconds\n",
      "Step 98 / 1000 - slice 17169:17346 DONE in 16.71680521965027 seconds\n",
      "Step 99 / 1000 - slice 17346:17523 DONE in 16.394519090652466 seconds\n",
      "Step 100 / 1000 - slice 17523:17700 DONE in 16.57802700996399 seconds\n",
      "Step 101 / 1000 - slice 17700:17877 DONE in 16.248669147491455 seconds\n",
      "Step 102 / 1000 - slice 17877:18054 DONE in 16.302336931228638 seconds\n",
      "Step 103 / 1000 - slice 18054:18231 DONE in 16.234431266784668 seconds\n",
      "Step 104 / 1000 - slice 18231:18408 DONE in 16.363553762435913 seconds\n",
      "Step 105 / 1000 - slice 18408:18585 DONE in 16.729851961135864 seconds\n",
      "Step 106 / 1000 - slice 18585:18762 DONE in 16.367261171340942 seconds\n",
      "Step 107 / 1000 - slice 18762:18939 DONE in 16.013805150985718 seconds\n",
      "Step 108 / 1000 - slice 18939:19116 DONE in 16.903104066848755 seconds\n",
      "Step 109 / 1000 - slice 19116:19293 DONE in 16.42380404472351 seconds\n",
      "Step 110 / 1000 - slice 19293:19470 DONE in 16.291563034057617 seconds\n",
      "Step 111 / 1000 - slice 19470:19647 DONE in 16.677964210510254 seconds\n",
      "Step 112 / 1000 - slice 19647:19824 DONE in 16.589353799819946 seconds\n",
      "Step 113 / 1000 - slice 19824:20001 DONE in 16.462132930755615 seconds\n",
      "Step 114 / 1000 - slice 20001:20178 DONE in 16.386422157287598 seconds\n",
      "Step 115 / 1000 - slice 20178:20355 DONE in 16.22010111808777 seconds\n",
      "Step 116 / 1000 - slice 20355:20532 DONE in 16.391270875930786 seconds\n",
      "Step 117 / 1000 - slice 20532:20709 DONE in 16.47003698348999 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 118 / 1000 - slice 20709:20886 DONE in 16.666855812072754 seconds\n",
      "Step 119 / 1000 - slice 20886:21063 DONE in 16.041518688201904 seconds\n",
      "Step 120 / 1000 - slice 21063:21240 DONE in 16.38516402244568 seconds\n",
      "Step 121 / 1000 - slice 21240:21417 DONE in 16.905516147613525 seconds\n",
      "Step 122 / 1000 - slice 21417:21594 DONE in 16.392373085021973 seconds\n",
      "Step 123 / 1000 - slice 21594:21771 DONE in 16.254162073135376 seconds\n",
      "Step 124 / 1000 - slice 21771:21948 DONE in 16.37112283706665 seconds\n",
      "Step 125 / 1000 - slice 21948:22125 DONE in 16.368615865707397 seconds\n",
      "Step 126 / 1000 - slice 22125:22302 DONE in 16.53879189491272 seconds\n",
      "Step 127 / 1000 - slice 22302:22479 DONE in 16.18482208251953 seconds\n",
      "Step 128 / 1000 - slice 22479:22656 DONE in 16.326942205429077 seconds\n",
      "Step 129 / 1000 - slice 22656:22833 DONE in 16.68998908996582 seconds\n",
      "Step 130 / 1000 - slice 22833:23010 DONE in 17.2572979927063 seconds\n",
      "Step 131 / 1000 - slice 23010:23187 DONE in 16.22209882736206 seconds\n",
      "Step 132 / 1000 - slice 23187:23364 DONE in 16.181591987609863 seconds\n",
      "Step 133 / 1000 - slice 23364:23541 DONE in 16.052555084228516 seconds\n",
      "Step 134 / 1000 - slice 23541:23718 DONE in 16.01135516166687 seconds\n",
      "Step 135 / 1000 - slice 23718:23895 DONE in 16.829978942871094 seconds\n",
      "Step 136 / 1000 - slice 23895:24072 DONE in 16.547565937042236 seconds\n",
      "Step 137 / 1000 - slice 24072:24249 DONE in 16.185375928878784 seconds\n",
      "Step 138 / 1000 - slice 24249:24426 DONE in 16.20159101486206 seconds\n",
      "Step 139 / 1000 - slice 24426:24603 DONE in 16.085997104644775 seconds\n",
      "Step 140 / 1000 - slice 24603:24780 DONE in 15.885081052780151 seconds\n",
      "Step 141 / 1000 - slice 24780:24957 DONE in 16.103209018707275 seconds\n",
      "Step 142 / 1000 - slice 24957:25134 DONE in 17.51910090446472 seconds\n",
      "Step 143 / 1000 - slice 25134:25311 DONE in 16.678452014923096 seconds\n",
      "Step 144 / 1000 - slice 25311:25488 DONE in 16.955219984054565 seconds\n",
      "Step 145 / 1000 - slice 25488:25665 DONE in 17.6177499294281 seconds\n",
      "Step 146 / 1000 - slice 25665:25842 DONE in 16.217779874801636 seconds\n",
      "Step 147 / 1000 - slice 25842:26019 DONE in 16.336710929870605 seconds\n",
      "Step 148 / 1000 - slice 26019:26196 DONE in 16.619792222976685 seconds\n",
      "Step 149 / 1000 - slice 26196:26373 DONE in 16.869853973388672 seconds\n",
      "Step 150 / 1000 - slice 26373:26550 DONE in 16.20008397102356 seconds\n",
      "Step 151 / 1000 - slice 26550:26727 DONE in 16.030884981155396 seconds\n",
      "Step 152 / 1000 - slice 26727:26904 DONE in 16.394647121429443 seconds\n",
      "Step 153 / 1000 - slice 26904:27081 DONE in 16.675594091415405 seconds\n",
      "Step 154 / 1000 - slice 27081:27258 DONE in 16.454494953155518 seconds\n",
      "Step 155 / 1000 - slice 27258:27435 DONE in 16.327205181121826 seconds\n",
      "Step 156 / 1000 - slice 27435:27612 DONE in 15.780367851257324 seconds\n",
      "Step 157 / 1000 - slice 27612:27789 DONE in 15.906844854354858 seconds\n",
      "Step 158 / 1000 - slice 27789:27966 DONE in 16.06942391395569 seconds\n",
      "Step 159 / 1000 - slice 27966:28143 DONE in 16.366735219955444 seconds\n",
      "Step 160 / 1000 - slice 28143:28320 DONE in 16.34954524040222 seconds\n",
      "Step 161 / 1000 - slice 28320:28497 DONE in 16.096946001052856 seconds\n",
      "Step 162 / 1000 - slice 28497:28674 DONE in 16.183887004852295 seconds\n",
      "Step 163 / 1000 - slice 28674:28851 DONE in 15.844802141189575 seconds\n",
      "Step 164 / 1000 - slice 28851:29028 DONE in 16.139172792434692 seconds\n",
      "Step 165 / 1000 - slice 29028:29205 DONE in 16.48047113418579 seconds\n",
      "Step 166 / 1000 - slice 29205:29382 DONE in 15.99288010597229 seconds\n",
      "Step 167 / 1000 - slice 29382:29559 DONE in 16.670505046844482 seconds\n",
      "Step 168 / 1000 - slice 29559:29736 DONE in 15.812463998794556 seconds\n",
      "Step 169 / 1000 - slice 29736:29913 DONE in 16.471156358718872 seconds\n",
      "Step 170 / 1000 - slice 29913:30090 DONE in 16.853668689727783 seconds\n",
      "Step 171 / 1000 - slice 30090:30267 DONE in 16.618284940719604 seconds\n",
      "Step 172 / 1000 - slice 30267:30444 DONE in 16.786870002746582 seconds\n",
      "Step 173 / 1000 - slice 30444:30621 DONE in 16.228874921798706 seconds\n",
      "Step 174 / 1000 - slice 30621:30798 DONE in 15.986572027206421 seconds\n",
      "Step 175 / 1000 - slice 30798:30975 DONE in 15.977816820144653 seconds\n",
      "Step 176 / 1000 - slice 30975:31152 DONE in 16.23228406906128 seconds\n",
      "Step 177 / 1000 - slice 31152:31329 DONE in 16.23895502090454 seconds\n",
      "Step 178 / 1000 - slice 31329:31506 DONE in 16.318538665771484 seconds\n",
      "Step 179 / 1000 - slice 31506:31683 DONE in 16.25102424621582 seconds\n",
      "Step 180 / 1000 - slice 31683:31860 DONE in 15.858141899108887 seconds\n",
      "Step 181 / 1000 - slice 31860:32037 DONE in 16.50104594230652 seconds\n",
      "Step 182 / 1000 - slice 32037:32214 DONE in 15.887817144393921 seconds\n",
      "Step 183 / 1000 - slice 32214:32391 DONE in 16.416486978530884 seconds\n",
      "Step 184 / 1000 - slice 32391:32568 DONE in 16.314459085464478 seconds\n",
      "Step 185 / 1000 - slice 32568:32745 DONE in 16.323078155517578 seconds\n",
      "Step 186 / 1000 - slice 32745:32922 DONE in 17.134127140045166 seconds\n",
      "Step 187 / 1000 - slice 32922:33099 DONE in 16.025673866271973 seconds\n",
      "Step 188 / 1000 - slice 33099:33276 DONE in 15.952226877212524 seconds\n",
      "Step 189 / 1000 - slice 33276:33453 DONE in 16.577807903289795 seconds\n",
      "Step 190 / 1000 - slice 33453:33630 DONE in 16.728520154953003 seconds\n",
      "Step 191 / 1000 - slice 33630:33807 DONE in 16.782557249069214 seconds\n",
      "Step 192 / 1000 - slice 33807:33984 DONE in 16.174208879470825 seconds\n",
      "Step 193 / 1000 - slice 33984:34161 DONE in 16.420630931854248 seconds\n",
      "Step 194 / 1000 - slice 34161:34338 DONE in 16.010149240493774 seconds\n",
      "Step 195 / 1000 - slice 34338:34515 DONE in 16.021920919418335 seconds\n",
      "Step 196 / 1000 - slice 34515:34692 DONE in 15.940173864364624 seconds\n",
      "Step 197 / 1000 - slice 34692:34869 DONE in 16.16772198677063 seconds\n",
      "Step 198 / 1000 - slice 34869:35046 DONE in 16.081851959228516 seconds\n",
      "Step 199 / 1000 - slice 35046:35223 DONE in 16.809952974319458 seconds\n",
      "Step 200 / 1000 - slice 35223:35400 DONE in 16.38773798942566 seconds\n",
      "Step 201 / 1000 - slice 35400:35577 DONE in 16.638296842575073 seconds\n",
      "Step 202 / 1000 - slice 35577:35754 DONE in 16.277608156204224 seconds\n",
      "Step 203 / 1000 - slice 35754:35931 DONE in 16.32703685760498 seconds\n",
      "Step 204 / 1000 - slice 35931:36108 DONE in 16.130923748016357 seconds\n",
      "Step 205 / 1000 - slice 36108:36285 DONE in 16.565855026245117 seconds\n",
      "Step 206 / 1000 - slice 36285:36462 DONE in 16.159741163253784 seconds\n",
      "Step 207 / 1000 - slice 36462:36639 DONE in 16.331977128982544 seconds\n",
      "Step 208 / 1000 - slice 36639:36816 DONE in 16.91591501235962 seconds\n",
      "Step 209 / 1000 - slice 36816:36993 DONE in 16.333189249038696 seconds\n",
      "Step 210 / 1000 - slice 36993:37170 DONE in 16.334528923034668 seconds\n",
      "Step 211 / 1000 - slice 37170:37347 DONE in 16.19802689552307 seconds\n",
      "Step 212 / 1000 - slice 37347:37524 DONE in 16.167768239974976 seconds\n",
      "Step 213 / 1000 - slice 37524:37701 DONE in 15.980993032455444 seconds\n",
      "Step 214 / 1000 - slice 37701:37878 DONE in 16.29823589324951 seconds\n",
      "Step 215 / 1000 - slice 37878:38055 DONE in 16.457396984100342 seconds\n",
      "Step 216 / 1000 - slice 38055:38232 DONE in 16.10741901397705 seconds\n",
      "Step 217 / 1000 - slice 38232:38409 DONE in 16.1947021484375 seconds\n",
      "Step 218 / 1000 - slice 38409:38586 DONE in 16.513033151626587 seconds\n",
      "Step 219 / 1000 - slice 38586:38763 DONE in 16.026818990707397 seconds\n",
      "Step 220 / 1000 - slice 38763:38940 DONE in 16.159835815429688 seconds\n",
      "Step 221 / 1000 - slice 38940:39117 DONE in 16.598548889160156 seconds\n",
      "Step 222 / 1000 - slice 39117:39294 DONE in 16.660489082336426 seconds\n",
      "Step 223 / 1000 - slice 39294:39471 DONE in 17.31658911705017 seconds\n",
      "Step 224 / 1000 - slice 39471:39648 DONE in 16.096373081207275 seconds\n",
      "Step 225 / 1000 - slice 39648:39825 DONE in 17.46922206878662 seconds\n",
      "Step 226 / 1000 - slice 39825:40002 DONE in 16.092042684555054 seconds\n",
      "Step 227 / 1000 - slice 40002:40179 DONE in 16.47494912147522 seconds\n",
      "Step 228 / 1000 - slice 40179:40356 DONE in 16.0870680809021 seconds\n",
      "Step 229 / 1000 - slice 40356:40533 DONE in 16.50711679458618 seconds\n",
      "Step 230 / 1000 - slice 40533:40710 DONE in 16.26284122467041 seconds\n",
      "Step 231 / 1000 - slice 40710:40887 DONE in 16.265393257141113 seconds\n",
      "Step 232 / 1000 - slice 40887:41064 DONE in 16.1767578125 seconds\n",
      "Step 233 / 1000 - slice 41064:41241 DONE in 16.158963203430176 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 234 / 1000 - slice 41241:41418 DONE in 16.16317105293274 seconds\n",
      "Step 235 / 1000 - slice 41418:41595 DONE in 16.83797287940979 seconds\n",
      "Step 236 / 1000 - slice 41595:41772 DONE in 16.031065940856934 seconds\n",
      "Step 237 / 1000 - slice 41772:41949 DONE in 16.576483011245728 seconds\n",
      "Step 238 / 1000 - slice 41949:42126 DONE in 16.404372930526733 seconds\n",
      "Step 239 / 1000 - slice 42126:42303 DONE in 16.510457038879395 seconds\n",
      "Step 240 / 1000 - slice 42303:42480 DONE in 16.37769865989685 seconds\n",
      "Step 241 / 1000 - slice 42480:42657 DONE in 16.394344329833984 seconds\n",
      "Step 242 / 1000 - slice 42657:42834 DONE in 17.637861013412476 seconds\n",
      "Step 243 / 1000 - slice 42834:43011 DONE in 16.319549083709717 seconds\n",
      "Step 244 / 1000 - slice 43011:43188 DONE in 16.005059957504272 seconds\n",
      "Step 245 / 1000 - slice 43188:43365 DONE in 16.652379989624023 seconds\n",
      "Step 246 / 1000 - slice 43365:43542 DONE in 18.34857487678528 seconds\n",
      "Step 247 / 1000 - slice 43542:43719 DONE in 17.06564497947693 seconds\n",
      "Step 248 / 1000 - slice 43719:43896 DONE in 16.515103101730347 seconds\n",
      "Step 249 / 1000 - slice 43896:44073 DONE in 16.355681657791138 seconds\n",
      "Step 250 / 1000 - slice 44073:44250 DONE in 16.209995985031128 seconds\n",
      "Step 251 / 1000 - slice 44250:44427 DONE in 16.27485704421997 seconds\n",
      "Step 252 / 1000 - slice 44427:44604 DONE in 16.461244821548462 seconds\n",
      "Step 253 / 1000 - slice 44604:44781 DONE in 16.746601819992065 seconds\n",
      "Step 254 / 1000 - slice 44781:44958 DONE in 16.73272705078125 seconds\n",
      "Step 255 / 1000 - slice 44958:45135 DONE in 16.54209566116333 seconds\n",
      "Step 256 / 1000 - slice 45135:45312 DONE in 16.309579133987427 seconds\n",
      "Step 257 / 1000 - slice 45312:45489 DONE in 18.970323085784912 seconds\n",
      "Step 258 / 1000 - slice 45489:45666 DONE in 18.249732971191406 seconds\n",
      "Step 259 / 1000 - slice 45666:45843 DONE in 16.23728108406067 seconds\n",
      "Step 260 / 1000 - slice 45843:46020 DONE in 17.056373834609985 seconds\n",
      "Step 261 / 1000 - slice 46020:46197 DONE in 16.59946918487549 seconds\n",
      "Step 262 / 1000 - slice 46197:46374 DONE in 16.99015498161316 seconds\n",
      "Step 263 / 1000 - slice 46374:46551 DONE in 16.44028902053833 seconds\n",
      "Step 264 / 1000 - slice 46551:46728 DONE in 16.34421730041504 seconds\n",
      "Step 265 / 1000 - slice 46728:46905 DONE in 16.515385150909424 seconds\n",
      "Step 266 / 1000 - slice 46905:47082 DONE in 16.681003093719482 seconds\n",
      "Step 267 / 1000 - slice 47082:47259 DONE in 16.284443140029907 seconds\n",
      "Step 268 / 1000 - slice 47259:47436 DONE in 17.034181118011475 seconds\n",
      "Step 269 / 1000 - slice 47436:47613 DONE in 16.650604009628296 seconds\n",
      "Step 270 / 1000 - slice 47613:47790 DONE in 17.367645978927612 seconds\n",
      "Step 271 / 1000 - slice 47790:47967 DONE in 16.674652814865112 seconds\n",
      "Step 272 / 1000 - slice 47967:48144 DONE in 16.437006950378418 seconds\n",
      "Step 273 / 1000 - slice 48144:48321 DONE in 16.53508496284485 seconds\n",
      "Step 274 / 1000 - slice 48321:48498 DONE in 16.673587799072266 seconds\n",
      "Step 275 / 1000 - slice 48498:48675 DONE in 17.286614894866943 seconds\n",
      "Step 276 / 1000 - slice 48675:48852 DONE in 17.15690279006958 seconds\n",
      "Step 277 / 1000 - slice 48852:49029 DONE in 16.65629005432129 seconds\n",
      "Step 278 / 1000 - slice 49029:49206 DONE in 16.815799951553345 seconds\n",
      "Step 279 / 1000 - slice 49206:49383 DONE in 16.480803728103638 seconds\n",
      "Step 280 / 1000 - slice 49383:49560 DONE in 16.927884101867676 seconds\n",
      "Step 281 / 1000 - slice 49560:49737 DONE in 16.4876389503479 seconds\n",
      "Step 282 / 1000 - slice 49737:49914 DONE in 16.280579090118408 seconds\n",
      "Step 283 / 1000 - slice 49914:50091 DONE in 16.240733861923218 seconds\n",
      "Step 284 / 1000 - slice 50091:50268 DONE in 16.555867195129395 seconds\n",
      "Step 285 / 1000 - slice 50268:50445 DONE in 16.67677617073059 seconds\n",
      "Step 286 / 1000 - slice 50445:50622 DONE in 17.263296127319336 seconds\n",
      "Step 287 / 1000 - slice 50622:50799 DONE in 16.77831220626831 seconds\n",
      "Step 288 / 1000 - slice 50799:50976 DONE in 17.010905265808105 seconds\n",
      "Step 289 / 1000 - slice 50976:51153 DONE in 16.56095004081726 seconds\n",
      "Step 290 / 1000 - slice 51153:51330 DONE in 16.40827775001526 seconds\n",
      "Step 291 / 1000 - slice 51330:51507 DONE in 16.330508708953857 seconds\n",
      "Step 292 / 1000 - slice 51507:51684 DONE in 16.359041929244995 seconds\n",
      "Step 293 / 1000 - slice 51684:51861 DONE in 17.60088086128235 seconds\n",
      "Step 294 / 1000 - slice 51861:52038 DONE in 16.651965856552124 seconds\n",
      "Step 295 / 1000 - slice 52038:52215 DONE in 16.403324127197266 seconds\n",
      "Step 296 / 1000 - slice 52215:52392 DONE in 16.1186740398407 seconds\n",
      "Step 297 / 1000 - slice 52392:52569 DONE in 16.449551820755005 seconds\n",
      "Step 298 / 1000 - slice 52569:52746 DONE in 16.59439516067505 seconds\n",
      "Step 299 / 1000 - slice 52746:52923 DONE in 16.306854009628296 seconds\n",
      "Step 300 / 1000 - slice 52923:53100 DONE in 16.664955854415894 seconds\n",
      "Step 301 / 1000 - slice 53100:53277 DONE in 16.307460069656372 seconds\n",
      "Step 302 / 1000 - slice 53277:53454 DONE in 16.171308040618896 seconds\n",
      "Step 303 / 1000 - slice 53454:53631 DONE in 16.20722270011902 seconds\n",
      "Step 304 / 1000 - slice 53631:53808 DONE in 16.022875785827637 seconds\n",
      "Step 305 / 1000 - slice 53808:53985 DONE in 16.172057151794434 seconds\n",
      "Step 306 / 1000 - slice 53985:54162 DONE in 16.317787170410156 seconds\n",
      "Step 307 / 1000 - slice 54162:54339 DONE in 27.24196481704712 seconds\n",
      "Step 308 / 1000 - slice 54339:54516 DONE in 18.466749906539917 seconds\n",
      "Step 309 / 1000 - slice 54516:54693 DONE in 18.67022395133972 seconds\n",
      "Step 310 / 1000 - slice 54693:54870 DONE in 14.441571950912476 seconds\n",
      "Step 311 / 1000 - slice 54870:55047 DONE in 12.320561170578003 seconds\n",
      "Step 312 / 1000 - slice 55047:55224 DONE in 11.886806726455688 seconds\n",
      "Step 313 / 1000 - slice 55224:55401 DONE in 11.710222959518433 seconds\n",
      "Step 314 / 1000 - slice 55401:55578 DONE in 11.387268304824829 seconds\n",
      "Step 315 / 1000 - slice 55578:55755 DONE in 11.43415093421936 seconds\n",
      "Step 316 / 1000 - slice 55755:55932 DONE in 11.176613807678223 seconds\n",
      "Step 317 / 1000 - slice 55932:56109 DONE in 12.816641092300415 seconds\n",
      "Step 318 / 1000 - slice 56109:56286 DONE in 10.99452805519104 seconds\n",
      "Step 319 / 1000 - slice 56286:56463 DONE in 11.014226198196411 seconds\n",
      "Step 320 / 1000 - slice 56463:56640 DONE in 10.970244884490967 seconds\n",
      "Step 321 / 1000 - slice 56640:56817 DONE in 11.057376861572266 seconds\n",
      "Step 322 / 1000 - slice 56817:56994 DONE in 11.083307027816772 seconds\n",
      "Step 323 / 1000 - slice 56994:57171 DONE in 11.840115070343018 seconds\n",
      "Step 324 / 1000 - slice 57171:57348 DONE in 11.811519861221313 seconds\n",
      "Step 325 / 1000 - slice 57348:57525 DONE in 11.173238754272461 seconds\n",
      "Step 326 / 1000 - slice 57525:57702 DONE in 11.044091939926147 seconds\n",
      "Step 327 / 1000 - slice 57702:57879 DONE in 11.150434970855713 seconds\n",
      "Step 328 / 1000 - slice 57879:58056 DONE in 11.41548490524292 seconds\n",
      "Step 329 / 1000 - slice 58056:58233 DONE in 11.235096216201782 seconds\n",
      "Step 330 / 1000 - slice 58233:58410 DONE in 10.99691891670227 seconds\n",
      "Step 331 / 1000 - slice 58410:58587 DONE in 11.065199136734009 seconds\n",
      "Step 332 / 1000 - slice 58587:58764 DONE in 10.975841999053955 seconds\n",
      "Step 333 / 1000 - slice 58764:58941 DONE in 11.110162019729614 seconds\n",
      "Step 334 / 1000 - slice 58941:59118 DONE in 12.17221212387085 seconds\n",
      "Step 335 / 1000 - slice 59118:59295 DONE in 12.946321964263916 seconds\n",
      "Step 336 / 1000 - slice 59295:59472 DONE in 12.13386607170105 seconds\n",
      "Step 337 / 1000 - slice 59472:59649 DONE in 14.51720380783081 seconds\n",
      "Step 338 / 1000 - slice 59649:59826 DONE in 15.781085968017578 seconds\n",
      "Step 339 / 1000 - slice 59826:60003 DONE in 14.795167922973633 seconds\n",
      "Step 340 / 1000 - slice 60003:60180 DONE in 12.838489055633545 seconds\n",
      "Step 341 / 1000 - slice 60180:60357 DONE in 13.285806894302368 seconds\n",
      "Step 342 / 1000 - slice 60357:60534 DONE in 11.936925888061523 seconds\n",
      "Step 343 / 1000 - slice 60534:60711 DONE in 11.93223786354065 seconds\n",
      "Step 344 / 1000 - slice 60711:60888 DONE in 11.672584056854248 seconds\n",
      "Step 345 / 1000 - slice 60888:61065 DONE in 11.757375001907349 seconds\n",
      "Step 346 / 1000 - slice 61065:61242 DONE in 11.775012969970703 seconds\n",
      "Step 347 / 1000 - slice 61242:61419 DONE in 12.224242925643921 seconds\n",
      "Step 348 / 1000 - slice 61419:61596 DONE in 11.517049074172974 seconds\n",
      "Step 349 / 1000 - slice 61596:61773 DONE in 11.654396057128906 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 350 / 1000 - slice 61773:61950 DONE in 11.506618738174438 seconds\n",
      "Step 351 / 1000 - slice 61950:62127 DONE in 11.491338014602661 seconds\n",
      "Step 352 / 1000 - slice 62127:62304 DONE in 11.135302782058716 seconds\n",
      "Step 353 / 1000 - slice 62304:62481 DONE in 11.176668882369995 seconds\n",
      "Step 354 / 1000 - slice 62481:62658 DONE in 11.197463035583496 seconds\n",
      "Step 355 / 1000 - slice 62658:62835 DONE in 11.159059047698975 seconds\n",
      "Step 356 / 1000 - slice 62835:63012 DONE in 11.18499207496643 seconds\n",
      "Step 357 / 1000 - slice 63012:63189 DONE in 11.150430917739868 seconds\n",
      "Step 358 / 1000 - slice 63189:63366 DONE in 11.087057828903198 seconds\n",
      "Step 359 / 1000 - slice 63366:63543 DONE in 11.11498498916626 seconds\n",
      "Step 360 / 1000 - slice 63543:63720 DONE in 11.129112005233765 seconds\n",
      "Step 361 / 1000 - slice 63720:63897 DONE in 13.060790777206421 seconds\n",
      "Step 362 / 1000 - slice 63897:64074 DONE in 11.180936813354492 seconds\n",
      "Step 363 / 1000 - slice 64074:64251 DONE in 11.112380981445312 seconds\n",
      "Step 364 / 1000 - slice 64251:64428 DONE in 11.106680870056152 seconds\n",
      "Step 365 / 1000 - slice 64428:64605 DONE in 12.483581066131592 seconds\n",
      "Step 366 / 1000 - slice 64605:64782 DONE in 12.672707080841064 seconds\n",
      "Step 367 / 1000 - slice 64782:64959 DONE in 12.166924953460693 seconds\n",
      "Step 368 / 1000 - slice 64959:65136 DONE in 13.545830249786377 seconds\n",
      "Step 369 / 1000 - slice 65136:65313 DONE in 12.249741077423096 seconds\n",
      "Step 370 / 1000 - slice 65313:65490 DONE in 17.011960744857788 seconds\n",
      "Step 371 / 1000 - slice 65490:65667 DONE in 12.402377843856812 seconds\n",
      "Step 372 / 1000 - slice 65667:65844 DONE in 12.651349067687988 seconds\n",
      "Step 373 / 1000 - slice 65844:66021 DONE in 11.31661581993103 seconds\n",
      "Step 374 / 1000 - slice 66021:66198 DONE in 12.885362148284912 seconds\n",
      "Step 375 / 1000 - slice 66198:66375 DONE in 11.104625940322876 seconds\n",
      "Step 376 / 1000 - slice 66375:66552 DONE in 12.716320991516113 seconds\n",
      "Step 377 / 1000 - slice 66552:66729 DONE in 13.056124925613403 seconds\n",
      "Step 378 / 1000 - slice 66729:66906 DONE in 13.557084798812866 seconds\n",
      "Step 379 / 1000 - slice 66906:67083 DONE in 11.894257307052612 seconds\n",
      "Step 380 / 1000 - slice 67083:67260 DONE in 13.11740517616272 seconds\n",
      "Step 381 / 1000 - slice 67260:67437 DONE in 11.888636350631714 seconds\n",
      "Step 382 / 1000 - slice 67437:67614 DONE in 12.22884202003479 seconds\n",
      "Step 383 / 1000 - slice 67614:67791 DONE in 11.479124069213867 seconds\n",
      "Step 384 / 1000 - slice 67791:67968 DONE in 11.296749114990234 seconds\n",
      "Step 385 / 1000 - slice 67968:68145 DONE in 11.007955074310303 seconds\n",
      "Step 386 / 1000 - slice 68145:68322 DONE in 11.081535816192627 seconds\n",
      "Step 387 / 1000 - slice 68322:68499 DONE in 11.026308059692383 seconds\n",
      "Step 388 / 1000 - slice 68499:68676 DONE in 10.986914873123169 seconds\n",
      "Step 389 / 1000 - slice 68676:68853 DONE in 11.078699588775635 seconds\n",
      "Step 390 / 1000 - slice 68853:69030 DONE in 12.15196180343628 seconds\n",
      "Step 391 / 1000 - slice 69030:69207 DONE in 12.349431991577148 seconds\n",
      "Step 392 / 1000 - slice 69207:69384 DONE in 13.7742280960083 seconds\n",
      "Step 393 / 1000 - slice 69384:69561 DONE in 13.953325033187866 seconds\n",
      "Step 394 / 1000 - slice 69561:69738 DONE in 12.78523588180542 seconds\n",
      "Step 395 / 1000 - slice 69738:69915 DONE in 12.102627038955688 seconds\n",
      "Step 396 / 1000 - slice 69915:70092 DONE in 13.640887022018433 seconds\n",
      "Step 397 / 1000 - slice 70092:70269 DONE in 13.271975994110107 seconds\n",
      "Step 398 / 1000 - slice 70269:70446 DONE in 12.864696025848389 seconds\n",
      "Step 399 / 1000 - slice 70446:70623 DONE in 13.062964916229248 seconds\n",
      "Step 400 / 1000 - slice 70623:70800 DONE in 13.695955991744995 seconds\n",
      "Step 401 / 1000 - slice 70800:70977 DONE in 12.470389127731323 seconds\n",
      "Step 402 / 1000 - slice 70977:71154 DONE in 12.83350396156311 seconds\n",
      "Step 403 / 1000 - slice 71154:71331 DONE in 13.505134105682373 seconds\n",
      "Step 404 / 1000 - slice 71331:71508 DONE in 13.009767055511475 seconds\n",
      "Step 405 / 1000 - slice 71508:71685 DONE in 12.64537501335144 seconds\n",
      "Step 406 / 1000 - slice 71685:71862 DONE in 12.58935284614563 seconds\n",
      "Step 407 / 1000 - slice 71862:72039 DONE in 12.635725736618042 seconds\n",
      "Step 408 / 1000 - slice 72039:72216 DONE in 12.771368980407715 seconds\n",
      "Step 409 / 1000 - slice 72216:72393 DONE in 12.931977033615112 seconds\n",
      "Step 410 / 1000 - slice 72393:72570 DONE in 12.502742767333984 seconds\n",
      "Step 411 / 1000 - slice 72570:72747 DONE in 13.42784595489502 seconds\n",
      "Step 412 / 1000 - slice 72747:72924 DONE in 12.367427110671997 seconds\n",
      "Step 413 / 1000 - slice 72924:73101 DONE in 16.066429376602173 seconds\n",
      "Step 414 / 1000 - slice 73101:73278 DONE in 16.64358401298523 seconds\n",
      "Step 415 / 1000 - slice 73278:73455 DONE in 15.441115140914917 seconds\n",
      "Step 416 / 1000 - slice 73455:73632 DONE in 12.05505895614624 seconds\n",
      "Step 417 / 1000 - slice 73632:73809 DONE in 12.194828033447266 seconds\n",
      "Step 418 / 1000 - slice 73809:73986 DONE in 16.325045108795166 seconds\n",
      "Step 419 / 1000 - slice 73986:74163 DONE in 15.964654684066772 seconds\n",
      "Step 420 / 1000 - slice 74163:74340 DONE in 14.225210189819336 seconds\n",
      "Step 421 / 1000 - slice 74340:74517 DONE in 13.91824197769165 seconds\n",
      "Step 422 / 1000 - slice 74517:74694 DONE in 14.333423852920532 seconds\n",
      "Step 423 / 1000 - slice 74694:74871 DONE in 13.116077184677124 seconds\n",
      "Step 424 / 1000 - slice 74871:75048 DONE in 11.699645042419434 seconds\n",
      "Step 425 / 1000 - slice 75048:75225 DONE in 11.352082967758179 seconds\n",
      "Step 426 / 1000 - slice 75225:75402 DONE in 11.38585114479065 seconds\n",
      "Step 427 / 1000 - slice 75402:75579 DONE in 11.260581016540527 seconds\n",
      "Step 428 / 1000 - slice 75579:75756 DONE in 11.486545085906982 seconds\n",
      "Step 429 / 1000 - slice 75756:75933 DONE in 11.567802667617798 seconds\n",
      "Step 430 / 1000 - slice 75933:76110 DONE in 11.36001992225647 seconds\n",
      "Step 431 / 1000 - slice 76110:76287 DONE in 11.34192705154419 seconds\n",
      "Step 432 / 1000 - slice 76287:76464 DONE in 11.358736038208008 seconds\n",
      "Step 433 / 1000 - slice 76464:76641 DONE in 11.27978801727295 seconds\n",
      "Step 434 / 1000 - slice 76641:76818 DONE in 11.372186183929443 seconds\n",
      "Step 435 / 1000 - slice 76818:76995 DONE in 11.165081024169922 seconds\n",
      "Step 436 / 1000 - slice 76995:77172 DONE in 11.899405002593994 seconds\n",
      "Step 437 / 1000 - slice 77172:77349 DONE in 12.135525703430176 seconds\n",
      "Step 438 / 1000 - slice 77349:77526 DONE in 13.491461992263794 seconds\n",
      "Step 439 / 1000 - slice 77526:77703 DONE in 11.030009984970093 seconds\n",
      "Step 440 / 1000 - slice 77703:77880 DONE in 11.766387939453125 seconds\n",
      "Step 441 / 1000 - slice 77880:78057 DONE in 11.146113872528076 seconds\n",
      "Step 442 / 1000 - slice 78057:78234 DONE in 10.966739177703857 seconds\n",
      "Step 443 / 1000 - slice 78234:78411 DONE in 11.077250003814697 seconds\n",
      "Step 444 / 1000 - slice 78411:78588 DONE in 10.889411211013794 seconds\n",
      "Step 445 / 1000 - slice 78588:78765 DONE in 12.135057210922241 seconds\n",
      "Step 446 / 1000 - slice 78765:78942 DONE in 12.094283103942871 seconds\n",
      "Step 447 / 1000 - slice 78942:79119 DONE in 14.22802209854126 seconds\n",
      "Step 448 / 1000 - slice 79119:79296 DONE in 11.799595832824707 seconds\n",
      "Step 449 / 1000 - slice 79296:79473 DONE in 11.00520920753479 seconds\n",
      "Step 450 / 1000 - slice 79473:79650 DONE in 10.955880880355835 seconds\n",
      "Step 451 / 1000 - slice 79650:79827 DONE in 11.088015794754028 seconds\n",
      "Step 452 / 1000 - slice 79827:80004 DONE in 10.885581016540527 seconds\n",
      "Step 453 / 1000 - slice 80004:80181 DONE in 10.892412900924683 seconds\n",
      "Step 454 / 1000 - slice 80181:80358 DONE in 10.88992428779602 seconds\n",
      "Step 455 / 1000 - slice 80358:80535 DONE in 11.964799880981445 seconds\n",
      "Step 456 / 1000 - slice 80535:80712 DONE in 11.39999508857727 seconds\n",
      "Step 457 / 1000 - slice 80712:80889 DONE in 12.1962308883667 seconds\n",
      "Step 458 / 1000 - slice 80889:81066 DONE in 11.434404134750366 seconds\n",
      "Step 459 / 1000 - slice 81066:81243 DONE in 12.034692764282227 seconds\n",
      "Step 460 / 1000 - slice 81243:81420 DONE in 12.031799077987671 seconds\n",
      "Step 461 / 1000 - slice 81420:81597 DONE in 13.548110008239746 seconds\n",
      "Step 462 / 1000 - slice 81597:81774 DONE in 12.679385900497437 seconds\n",
      "Step 463 / 1000 - slice 81774:81951 DONE in 11.676679849624634 seconds\n",
      "Step 464 / 1000 - slice 81951:82128 DONE in 11.094669818878174 seconds\n",
      "Step 465 / 1000 - slice 82128:82305 DONE in 15.33676791191101 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 466 / 1000 - slice 82305:82482 DONE in 19.703739166259766 seconds\n",
      "Step 467 / 1000 - slice 82482:82659 DONE in 13.292970895767212 seconds\n",
      "Step 468 / 1000 - slice 82659:82836 DONE in 13.57598090171814 seconds\n",
      "Step 469 / 1000 - slice 82836:83013 DONE in 12.759927988052368 seconds\n",
      "Step 470 / 1000 - slice 83013:83190 DONE in 11.158961057662964 seconds\n",
      "Step 471 / 1000 - slice 83190:83367 DONE in 11.031543016433716 seconds\n",
      "Step 472 / 1000 - slice 83367:83544 DONE in 11.237030982971191 seconds\n",
      "Step 473 / 1000 - slice 83544:83721 DONE in 10.94584584236145 seconds\n",
      "Step 474 / 1000 - slice 83721:83898 DONE in 10.938810110092163 seconds\n",
      "Step 475 / 1000 - slice 83898:84075 DONE in 10.935195922851562 seconds\n",
      "Step 476 / 1000 - slice 84075:84252 DONE in 10.909494161605835 seconds\n",
      "Step 477 / 1000 - slice 84252:84429 DONE in 11.349190950393677 seconds\n",
      "Step 478 / 1000 - slice 84429:84606 DONE in 10.946097135543823 seconds\n",
      "Step 479 / 1000 - slice 84606:84783 DONE in 10.95789098739624 seconds\n",
      "Step 480 / 1000 - slice 84783:84960 DONE in 10.86830472946167 seconds\n",
      "Step 481 / 1000 - slice 84960:85137 DONE in 11.04800796508789 seconds\n",
      "Step 482 / 1000 - slice 85137:85314 DONE in 10.951732158660889 seconds\n",
      "Step 483 / 1000 - slice 85314:85491 DONE in 10.951227188110352 seconds\n",
      "Step 484 / 1000 - slice 85491:85668 DONE in 10.95347809791565 seconds\n",
      "Step 485 / 1000 - slice 85668:85845 DONE in 14.53848385810852 seconds\n",
      "Step 486 / 1000 - slice 85845:86022 DONE in 12.33995008468628 seconds\n",
      "Step 487 / 1000 - slice 86022:86199 DONE in 12.337966918945312 seconds\n",
      "Step 488 / 1000 - slice 86199:86376 DONE in 11.042193174362183 seconds\n",
      "Step 489 / 1000 - slice 86376:86553 DONE in 11.010862112045288 seconds\n",
      "Step 490 / 1000 - slice 86553:86730 DONE in 11.027491092681885 seconds\n",
      "Step 491 / 1000 - slice 86730:86907 DONE in 10.906352043151855 seconds\n",
      "Step 492 / 1000 - slice 86907:87084 DONE in 12.125119924545288 seconds\n",
      "Step 493 / 1000 - slice 87084:87261 DONE in 11.28590703010559 seconds\n",
      "Step 494 / 1000 - slice 87261:87438 DONE in 10.975368022918701 seconds\n",
      "Step 495 / 1000 - slice 87438:87615 DONE in 11.4491548538208 seconds\n",
      "Step 496 / 1000 - slice 87615:87792 DONE in 12.350680112838745 seconds\n",
      "Step 497 / 1000 - slice 87792:87969 DONE in 14.628255844116211 seconds\n",
      "Step 498 / 1000 - slice 87969:88146 DONE in 13.988798141479492 seconds\n",
      "Step 499 / 1000 - slice 88146:88323 DONE in 11.230350017547607 seconds\n",
      "Step 500 / 1000 - slice 88323:88500 DONE in 11.018038034439087 seconds\n",
      "Step 501 / 1000 - slice 88500:88677 DONE in 11.002301931381226 seconds\n",
      "Step 502 / 1000 - slice 88677:88854 DONE in 11.370556831359863 seconds\n",
      "Step 503 / 1000 - slice 88854:89031 DONE in 11.135271787643433 seconds\n",
      "Step 504 / 1000 - slice 89031:89208 DONE in 11.186936140060425 seconds\n",
      "Step 505 / 1000 - slice 89208:89385 DONE in 10.929289817810059 seconds\n",
      "Step 506 / 1000 - slice 89385:89562 DONE in 11.033528804779053 seconds\n",
      "Step 507 / 1000 - slice 89562:89739 DONE in 11.095258951187134 seconds\n",
      "Step 508 / 1000 - slice 89739:89916 DONE in 10.999084234237671 seconds\n",
      "Step 509 / 1000 - slice 89916:90093 DONE in 11.043555974960327 seconds\n",
      "Step 510 / 1000 - slice 90093:90270 DONE in 10.968549013137817 seconds\n",
      "Step 511 / 1000 - slice 90270:90447 DONE in 10.988238096237183 seconds\n",
      "Step 512 / 1000 - slice 90447:90624 DONE in 11.311367988586426 seconds\n",
      "Step 513 / 1000 - slice 90624:90801 DONE in 12.000816822052002 seconds\n",
      "Step 514 / 1000 - slice 90801:90978 DONE in 13.870720863342285 seconds\n",
      "Step 515 / 1000 - slice 90978:91155 DONE in 11.568702936172485 seconds\n",
      "Step 516 / 1000 - slice 91155:91332 DONE in 12.256909132003784 seconds\n",
      "Step 517 / 1000 - slice 91332:91509 DONE in 10.876132011413574 seconds\n",
      "Step 518 / 1000 - slice 91509:91686 DONE in 11.900466203689575 seconds\n",
      "Step 519 / 1000 - slice 91686:91863 DONE in 11.520932912826538 seconds\n",
      "Step 520 / 1000 - slice 91863:92040 DONE in 11.833486080169678 seconds\n",
      "Step 521 / 1000 - slice 92040:92217 DONE in 12.80664873123169 seconds\n",
      "Step 522 / 1000 - slice 92217:92394 DONE in 13.771160125732422 seconds\n",
      "Step 523 / 1000 - slice 92394:92571 DONE in 20.420482873916626 seconds\n",
      "Step 524 / 1000 - slice 92571:92748 DONE in 12.887008905410767 seconds\n",
      "Step 525 / 1000 - slice 92748:92925 DONE in 15.68286395072937 seconds\n",
      "Step 526 / 1000 - slice 92925:93102 DONE in 11.988040924072266 seconds\n",
      "Step 527 / 1000 - slice 93102:93279 DONE in 12.069761753082275 seconds\n",
      "Step 528 / 1000 - slice 93279:93456 DONE in 12.643003940582275 seconds\n",
      "Step 529 / 1000 - slice 93456:93633 DONE in 15.633908987045288 seconds\n",
      "Step 530 / 1000 - slice 93633:93810 DONE in 11.23497486114502 seconds\n",
      "Step 531 / 1000 - slice 93810:93987 DONE in 11.442277908325195 seconds\n",
      "Step 532 / 1000 - slice 93987:94164 DONE in 11.880772113800049 seconds\n",
      "Step 533 / 1000 - slice 94164:94341 DONE in 13.914196968078613 seconds\n",
      "Step 534 / 1000 - slice 94341:94518 DONE in 13.259298086166382 seconds\n",
      "Step 535 / 1000 - slice 94518:94695 DONE in 13.060647964477539 seconds\n",
      "Step 536 / 1000 - slice 94695:94872 DONE in 11.764475107192993 seconds\n",
      "Step 537 / 1000 - slice 94872:95049 DONE in 12.37193512916565 seconds\n",
      "Step 538 / 1000 - slice 95049:95226 DONE in 12.178488969802856 seconds\n",
      "Step 539 / 1000 - slice 95226:95403 DONE in 11.742992877960205 seconds\n",
      "Step 540 / 1000 - slice 95403:95580 DONE in 12.1371009349823 seconds\n",
      "Step 541 / 1000 - slice 95580:95757 DONE in 11.443081140518188 seconds\n",
      "Step 542 / 1000 - slice 95757:95934 DONE in 13.172297954559326 seconds\n",
      "Step 543 / 1000 - slice 95934:96111 DONE in 12.688132047653198 seconds\n",
      "Step 544 / 1000 - slice 96111:96288 DONE in 11.270215034484863 seconds\n",
      "Step 545 / 1000 - slice 96288:96465 DONE in 11.488272905349731 seconds\n",
      "Step 546 / 1000 - slice 96465:96642 DONE in 11.104991912841797 seconds\n",
      "Step 547 / 1000 - slice 96642:96819 DONE in 11.100510835647583 seconds\n",
      "Step 548 / 1000 - slice 96819:96996 DONE in 11.873549938201904 seconds\n",
      "Step 549 / 1000 - slice 96996:97173 DONE in 11.015177011489868 seconds\n",
      "Step 550 / 1000 - slice 97173:97350 DONE in 10.842506885528564 seconds\n",
      "Step 551 / 1000 - slice 97350:97527 DONE in 11.063808917999268 seconds\n",
      "Step 552 / 1000 - slice 97527:97704 DONE in 11.076617002487183 seconds\n",
      "Step 553 / 1000 - slice 97704:97881 DONE in 10.937832832336426 seconds\n",
      "Step 554 / 1000 - slice 97881:98058 DONE in 11.139353036880493 seconds\n",
      "Step 555 / 1000 - slice 98058:98235 DONE in 11.004431009292603 seconds\n",
      "Step 556 / 1000 - slice 98235:98412 DONE in 10.967993974685669 seconds\n",
      "Step 557 / 1000 - slice 98412:98589 DONE in 12.766651153564453 seconds\n",
      "Step 558 / 1000 - slice 98589:98766 DONE in 14.067466974258423 seconds\n",
      "Step 559 / 1000 - slice 98766:98943 DONE in 11.111184120178223 seconds\n",
      "Step 560 / 1000 - slice 98943:99120 DONE in 10.869075775146484 seconds\n",
      "Step 561 / 1000 - slice 99120:99297 DONE in 12.794747114181519 seconds\n",
      "Step 562 / 1000 - slice 99297:99474 DONE in 12.127613067626953 seconds\n",
      "Step 563 / 1000 - slice 99474:99651 DONE in 11.01510500907898 seconds\n",
      "Step 564 / 1000 - slice 99651:99828 DONE in 12.557651042938232 seconds\n",
      "Step 565 / 1000 - slice 99828:100005 DONE in 13.385107040405273 seconds\n",
      "Step 566 / 1000 - slice 100005:100182 DONE in 13.320971012115479 seconds\n",
      "Step 567 / 1000 - slice 100182:100359 DONE in 11.970815181732178 seconds\n",
      "Step 568 / 1000 - slice 100359:100536 DONE in 12.245641946792603 seconds\n",
      "Step 569 / 1000 - slice 100536:100713 DONE in 13.18370509147644 seconds\n",
      "Step 570 / 1000 - slice 100713:100890 DONE in 11.066205024719238 seconds\n",
      "Step 571 / 1000 - slice 100890:101067 DONE in 13.534467935562134 seconds\n",
      "Step 572 / 1000 - slice 101067:101244 DONE in 13.867980003356934 seconds\n",
      "Step 573 / 1000 - slice 101244:101421 DONE in 14.64902114868164 seconds\n",
      "Step 574 / 1000 - slice 101421:101598 DONE in 13.622293949127197 seconds\n",
      "Step 575 / 1000 - slice 101598:101775 DONE in 11.567838907241821 seconds\n",
      "Step 576 / 1000 - slice 101775:101952 DONE in 11.53221607208252 seconds\n",
      "Step 577 / 1000 - slice 101952:102129 DONE in 12.46938419342041 seconds\n",
      "Step 578 / 1000 - slice 102129:102306 DONE in 11.386672019958496 seconds\n",
      "Step 579 / 1000 - slice 102306:102483 DONE in 11.470472812652588 seconds\n",
      "Step 580 / 1000 - slice 102483:102660 DONE in 11.565222024917603 seconds\n",
      "Step 581 / 1000 - slice 102660:102837 DONE in 11.356339931488037 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 582 / 1000 - slice 102837:103014 DONE in 11.298645973205566 seconds\n",
      "Step 583 / 1000 - slice 103014:103191 DONE in 12.458064794540405 seconds\n",
      "Step 584 / 1000 - slice 103191:103368 DONE in 12.254891872406006 seconds\n",
      "Step 585 / 1000 - slice 103368:103545 DONE in 12.81511902809143 seconds\n",
      "Step 586 / 1000 - slice 103545:103722 DONE in 11.999859809875488 seconds\n",
      "Step 587 / 1000 - slice 103722:103899 DONE in 13.078321933746338 seconds\n",
      "Step 588 / 1000 - slice 103899:104076 DONE in 11.160415887832642 seconds\n",
      "Step 589 / 1000 - slice 104076:104253 DONE in 10.93601393699646 seconds\n",
      "Step 590 / 1000 - slice 104253:104430 DONE in 11.105652093887329 seconds\n",
      "Step 591 / 1000 - slice 104430:104607 DONE in 11.078790187835693 seconds\n",
      "Step 592 / 1000 - slice 104607:104784 DONE in 10.814877986907959 seconds\n",
      "Step 593 / 1000 - slice 104784:104961 DONE in 13.698939323425293 seconds\n",
      "Step 594 / 1000 - slice 104961:105138 DONE in 12.422838926315308 seconds\n",
      "Step 595 / 1000 - slice 105138:105315 DONE in 12.650939226150513 seconds\n",
      "Step 596 / 1000 - slice 105315:105492 DONE in 10.985084056854248 seconds\n",
      "Step 597 / 1000 - slice 105492:105669 DONE in 11.014549016952515 seconds\n",
      "Step 598 / 1000 - slice 105669:105846 DONE in 11.064943790435791 seconds\n",
      "Step 599 / 1000 - slice 105846:106023 DONE in 10.928272008895874 seconds\n",
      "Step 600 / 1000 - slice 106023:106200 DONE in 11.118570327758789 seconds\n",
      "Step 601 / 1000 - slice 106200:106377 DONE in 11.14903998374939 seconds\n",
      "Step 602 / 1000 - slice 106377:106554 DONE in 10.961964130401611 seconds\n",
      "Step 603 / 1000 - slice 106554:106731 DONE in 11.064961910247803 seconds\n",
      "Step 604 / 1000 - slice 106731:106908 DONE in 11.103194952011108 seconds\n",
      "Step 605 / 1000 - slice 106908:107085 DONE in 11.401745796203613 seconds\n",
      "Step 606 / 1000 - slice 107085:107262 DONE in 11.568108081817627 seconds\n",
      "Step 607 / 1000 - slice 107262:107439 DONE in 11.431384086608887 seconds\n",
      "Step 608 / 1000 - slice 107439:107616 DONE in 11.186530113220215 seconds\n",
      "Step 609 / 1000 - slice 107616:107793 DONE in 11.52833104133606 seconds\n",
      "Step 610 / 1000 - slice 107793:107970 DONE in 11.429356098175049 seconds\n",
      "Step 611 / 1000 - slice 107970:108147 DONE in 11.008322954177856 seconds\n",
      "Step 612 / 1000 - slice 108147:108324 DONE in 11.144607067108154 seconds\n",
      "Step 613 / 1000 - slice 108324:108501 DONE in 10.966524124145508 seconds\n",
      "Step 614 / 1000 - slice 108501:108678 DONE in 11.204739093780518 seconds\n",
      "Step 615 / 1000 - slice 108678:108855 DONE in 11.02463698387146 seconds\n",
      "Step 616 / 1000 - slice 108855:109032 DONE in 11.001549005508423 seconds\n",
      "Step 617 / 1000 - slice 109032:109209 DONE in 10.995826959609985 seconds\n",
      "Step 618 / 1000 - slice 109209:109386 DONE in 11.217459201812744 seconds\n",
      "Step 619 / 1000 - slice 109386:109563 DONE in 11.191927194595337 seconds\n",
      "Step 620 / 1000 - slice 109563:109740 DONE in 11.759516954421997 seconds\n",
      "Step 621 / 1000 - slice 109740:109917 DONE in 11.011479139328003 seconds\n",
      "Step 622 / 1000 - slice 109917:110094 DONE in 11.065269947052002 seconds\n",
      "Step 623 / 1000 - slice 110094:110271 DONE in 11.019145011901855 seconds\n",
      "Step 624 / 1000 - slice 110271:110448 DONE in 10.995505809783936 seconds\n",
      "Step 625 / 1000 - slice 110448:110625 DONE in 11.539345741271973 seconds\n",
      "Step 626 / 1000 - slice 110625:110802 DONE in 11.095181941986084 seconds\n",
      "Step 627 / 1000 - slice 110802:110979 DONE in 11.067974090576172 seconds\n",
      "Step 628 / 1000 - slice 110979:111156 DONE in 11.76710033416748 seconds\n",
      "Step 629 / 1000 - slice 111156:111333 DONE in 11.147737979888916 seconds\n",
      "Step 630 / 1000 - slice 111333:111510 DONE in 11.026217937469482 seconds\n",
      "Step 631 / 1000 - slice 111510:111687 DONE in 10.893074750900269 seconds\n",
      "Step 632 / 1000 - slice 111687:111864 DONE in 10.962981224060059 seconds\n",
      "Step 633 / 1000 - slice 111864:112041 DONE in 10.947395086288452 seconds\n",
      "Step 634 / 1000 - slice 112041:112218 DONE in 11.835461139678955 seconds\n",
      "Step 635 / 1000 - slice 112218:112395 DONE in 12.560465097427368 seconds\n",
      "Step 636 / 1000 - slice 112395:112572 DONE in 12.143471956253052 seconds\n",
      "Step 637 / 1000 - slice 112572:112749 DONE in 10.895639181137085 seconds\n",
      "Step 638 / 1000 - slice 112749:112926 DONE in 10.992172002792358 seconds\n",
      "Step 639 / 1000 - slice 112926:113103 DONE in 10.824156045913696 seconds\n",
      "Step 640 / 1000 - slice 113103:113280 DONE in 10.837408065795898 seconds\n",
      "Step 641 / 1000 - slice 113280:113457 DONE in 10.855998992919922 seconds\n",
      "Step 642 / 1000 - slice 113457:113634 DONE in 11.025897026062012 seconds\n",
      "Step 643 / 1000 - slice 113634:113811 DONE in 11.003547191619873 seconds\n",
      "Step 644 / 1000 - slice 113811:113988 DONE in 10.831818103790283 seconds\n",
      "Step 645 / 1000 - slice 113988:114165 DONE in 11.07909607887268 seconds\n",
      "Step 646 / 1000 - slice 114165:114342 DONE in 10.942907333374023 seconds\n",
      "Step 647 / 1000 - slice 114342:114519 DONE in 10.919358968734741 seconds\n",
      "Step 648 / 1000 - slice 114519:114696 DONE in 10.956601858139038 seconds\n",
      "Step 649 / 1000 - slice 114696:114873 DONE in 10.97390103340149 seconds\n",
      "Step 650 / 1000 - slice 114873:115050 DONE in 11.050333023071289 seconds\n",
      "Step 651 / 1000 - slice 115050:115227 DONE in 10.98892092704773 seconds\n",
      "Step 652 / 1000 - slice 115227:115404 DONE in 10.942105770111084 seconds\n",
      "Step 653 / 1000 - slice 115404:115581 DONE in 10.924814939498901 seconds\n",
      "Step 654 / 1000 - slice 115581:115758 DONE in 11.119267702102661 seconds\n",
      "Step 655 / 1000 - slice 115758:115935 DONE in 10.993088006973267 seconds\n",
      "Step 656 / 1000 - slice 115935:116112 DONE in 10.924020290374756 seconds\n",
      "Step 657 / 1000 - slice 116112:116289 DONE in 11.1024649143219 seconds\n",
      "Step 658 / 1000 - slice 116289:116466 DONE in 10.940035820007324 seconds\n",
      "Step 659 / 1000 - slice 116466:116643 DONE in 10.93469524383545 seconds\n",
      "Step 660 / 1000 - slice 116643:116820 DONE in 10.992152214050293 seconds\n",
      "Step 661 / 1000 - slice 116820:116997 DONE in 10.909356117248535 seconds\n",
      "Step 662 / 1000 - slice 116997:117174 DONE in 11.409907817840576 seconds\n",
      "Step 663 / 1000 - slice 117174:117351 DONE in 10.941871881484985 seconds\n",
      "Step 664 / 1000 - slice 117351:117528 DONE in 12.00779104232788 seconds\n",
      "Step 665 / 1000 - slice 117528:117705 DONE in 11.412977933883667 seconds\n",
      "Step 666 / 1000 - slice 117705:117882 DONE in 10.986116170883179 seconds\n",
      "Step 667 / 1000 - slice 117882:118059 DONE in 10.864651918411255 seconds\n",
      "Step 668 / 1000 - slice 118059:118236 DONE in 10.887708187103271 seconds\n",
      "Step 669 / 1000 - slice 118236:118413 DONE in 11.111340999603271 seconds\n",
      "Step 670 / 1000 - slice 118413:118590 DONE in 10.989326238632202 seconds\n",
      "Step 671 / 1000 - slice 118590:118767 DONE in 10.90986704826355 seconds\n",
      "Step 672 / 1000 - slice 118767:118944 DONE in 10.925027847290039 seconds\n",
      "Step 673 / 1000 - slice 118944:119121 DONE in 11.010882139205933 seconds\n",
      "Step 674 / 1000 - slice 119121:119298 DONE in 10.89224910736084 seconds\n",
      "Step 675 / 1000 - slice 119298:119475 DONE in 10.919118165969849 seconds\n",
      "Step 676 / 1000 - slice 119475:119652 DONE in 10.866938829421997 seconds\n",
      "Step 677 / 1000 - slice 119652:119829 DONE in 10.885891675949097 seconds\n",
      "Step 678 / 1000 - slice 119829:120006 DONE in 10.835074186325073 seconds\n",
      "Step 679 / 1000 - slice 120006:120183 DONE in 11.16356110572815 seconds\n",
      "Step 680 / 1000 - slice 120183:120360 DONE in 12.1355881690979 seconds\n",
      "Step 681 / 1000 - slice 120360:120537 DONE in 12.457816123962402 seconds\n",
      "Step 682 / 1000 - slice 120537:120714 DONE in 13.157685995101929 seconds\n",
      "Step 683 / 1000 - slice 120714:120891 DONE in 12.953341960906982 seconds\n",
      "Step 684 / 1000 - slice 120891:121068 DONE in 12.455289125442505 seconds\n",
      "Step 685 / 1000 - slice 121068:121245 DONE in 12.815339088439941 seconds\n",
      "Step 686 / 1000 - slice 121245:121422 DONE in 12.451165914535522 seconds\n",
      "Step 687 / 1000 - slice 121422:121599 DONE in 12.82009768486023 seconds\n",
      "Step 688 / 1000 - slice 121599:121776 DONE in 12.39054012298584 seconds\n",
      "Step 689 / 1000 - slice 121776:121953 DONE in 12.573276281356812 seconds\n",
      "Step 690 / 1000 - slice 121953:122130 DONE in 12.740506172180176 seconds\n",
      "Step 691 / 1000 - slice 122130:122307 DONE in 13.535763025283813 seconds\n",
      "Step 692 / 1000 - slice 122307:122484 DONE in 12.229604244232178 seconds\n",
      "Step 693 / 1000 - slice 122484:122661 DONE in 12.255972862243652 seconds\n",
      "Step 694 / 1000 - slice 122661:122838 DONE in 12.38440990447998 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 695 / 1000 - slice 122838:123015 DONE in 12.269747018814087 seconds\n",
      "Step 696 / 1000 - slice 123015:123192 DONE in 12.167454957962036 seconds\n",
      "Step 697 / 1000 - slice 123192:123369 DONE in 12.245534896850586 seconds\n",
      "Step 698 / 1000 - slice 123369:123546 DONE in 12.180423974990845 seconds\n",
      "Step 699 / 1000 - slice 123546:123723 DONE in 12.155263900756836 seconds\n",
      "Step 700 / 1000 - slice 123723:123900 DONE in 12.27121615409851 seconds\n",
      "Step 701 / 1000 - slice 123900:124077 DONE in 12.205442905426025 seconds\n",
      "Step 702 / 1000 - slice 124077:124254 DONE in 12.271299123764038 seconds\n",
      "Step 703 / 1000 - slice 124254:124431 DONE in 12.439842224121094 seconds\n",
      "Step 704 / 1000 - slice 124431:124608 DONE in 12.148783206939697 seconds\n",
      "Step 705 / 1000 - slice 124608:124785 DONE in 12.262766122817993 seconds\n",
      "Step 706 / 1000 - slice 124785:124962 DONE in 12.436716079711914 seconds\n",
      "Step 707 / 1000 - slice 124962:125139 DONE in 12.311320781707764 seconds\n",
      "Step 708 / 1000 - slice 125139:125316 DONE in 12.212679862976074 seconds\n",
      "Step 709 / 1000 - slice 125316:125493 DONE in 13.568816184997559 seconds\n",
      "Step 710 / 1000 - slice 125493:125670 DONE in 11.033318042755127 seconds\n",
      "Step 711 / 1000 - slice 125670:125847 DONE in 11.923413038253784 seconds\n",
      "Step 712 / 1000 - slice 125847:126024 DONE in 10.994319200515747 seconds\n",
      "Step 713 / 1000 - slice 126024:126201 DONE in 11.663037061691284 seconds\n",
      "Step 714 / 1000 - slice 126201:126378 DONE in 12.30935287475586 seconds\n",
      "Step 715 / 1000 - slice 126378:126555 DONE in 12.94998288154602 seconds\n",
      "Step 716 / 1000 - slice 126555:126732 DONE in 12.363940000534058 seconds\n",
      "Step 717 / 1000 - slice 126732:126909 DONE in 11.073592185974121 seconds\n",
      "Step 718 / 1000 - slice 126909:127086 DONE in 12.3349027633667 seconds\n",
      "Step 719 / 1000 - slice 127086:127263 DONE in 10.86435580253601 seconds\n",
      "Step 720 / 1000 - slice 127263:127440 DONE in 15.121033906936646 seconds\n",
      "Step 721 / 1000 - slice 127440:127617 DONE in 11.83597993850708 seconds\n",
      "Step 722 / 1000 - slice 127617:127794 DONE in 14.86188006401062 seconds\n",
      "Step 723 / 1000 - slice 127794:127971 DONE in 13.528067111968994 seconds\n",
      "Step 724 / 1000 - slice 127971:128148 DONE in 13.014886856079102 seconds\n",
      "Step 725 / 1000 - slice 128148:128325 DONE in 12.228072881698608 seconds\n",
      "Step 726 / 1000 - slice 128325:128502 DONE in 11.074004173278809 seconds\n",
      "Step 727 / 1000 - slice 128502:128679 DONE in 11.415808200836182 seconds\n",
      "Step 728 / 1000 - slice 128679:128856 DONE in 11.2673499584198 seconds\n",
      "Step 729 / 1000 - slice 128856:129033 DONE in 11.17427110671997 seconds\n",
      "Step 730 / 1000 - slice 129033:129210 DONE in 11.05761981010437 seconds\n",
      "Step 731 / 1000 - slice 129210:129387 DONE in 11.035072088241577 seconds\n",
      "Step 732 / 1000 - slice 129387:129564 DONE in 11.144364833831787 seconds\n",
      "Step 733 / 1000 - slice 129564:129741 DONE in 11.127261877059937 seconds\n",
      "Step 734 / 1000 - slice 129741:129918 DONE in 11.215323209762573 seconds\n",
      "Step 735 / 1000 - slice 129918:130095 DONE in 12.653791189193726 seconds\n",
      "Step 736 / 1000 - slice 130095:130272 DONE in 17.492605924606323 seconds\n",
      "Step 737 / 1000 - slice 130272:130449 DONE in 12.976825952529907 seconds\n",
      "Step 738 / 1000 - slice 130449:130626 DONE in 12.448177099227905 seconds\n",
      "Step 739 / 1000 - slice 130626:130803 DONE in 11.127701759338379 seconds\n",
      "Step 740 / 1000 - slice 130803:130980 DONE in 11.833847999572754 seconds\n",
      "Step 741 / 1000 - slice 130980:131157 DONE in 15.713207960128784 seconds\n",
      "Step 742 / 1000 - slice 131157:131334 DONE in 12.14283275604248 seconds\n",
      "Step 743 / 1000 - slice 131334:131511 DONE in 10.927603244781494 seconds\n",
      "Step 744 / 1000 - slice 131511:131688 DONE in 10.978893995285034 seconds\n",
      "Step 745 / 1000 - slice 131688:131865 DONE in 10.938594102859497 seconds\n",
      "Step 746 / 1000 - slice 131865:132042 DONE in 11.052710771560669 seconds\n",
      "Step 747 / 1000 - slice 132042:132219 DONE in 10.960670948028564 seconds\n",
      "Step 748 / 1000 - slice 132219:132396 DONE in 11.58857011795044 seconds\n",
      "Step 749 / 1000 - slice 132396:132573 DONE in 11.187423944473267 seconds\n",
      "Step 750 / 1000 - slice 132573:132750 DONE in 11.196164846420288 seconds\n",
      "Step 751 / 1000 - slice 132750:132927 DONE in 11.306113958358765 seconds\n",
      "Step 752 / 1000 - slice 132927:133104 DONE in 11.074188709259033 seconds\n",
      "Step 753 / 1000 - slice 133104:133281 DONE in 10.923428297042847 seconds\n",
      "Step 754 / 1000 - slice 133281:133458 DONE in 10.98171615600586 seconds\n",
      "Step 755 / 1000 - slice 133458:133635 DONE in 11.013989925384521 seconds\n",
      "Step 756 / 1000 - slice 133635:133812 DONE in 12.371187925338745 seconds\n",
      "Step 757 / 1000 - slice 133812:133989 DONE in 14.784239053726196 seconds\n",
      "Step 758 / 1000 - slice 133989:134166 DONE in 14.68429183959961 seconds\n",
      "Step 759 / 1000 - slice 134166:134343 DONE in 12.870618104934692 seconds\n",
      "Step 760 / 1000 - slice 134343:134520 DONE in 13.747220039367676 seconds\n",
      "Step 761 / 1000 - slice 134520:134697 DONE in 14.947681903839111 seconds\n",
      "Step 762 / 1000 - slice 134697:134874 DONE in 11.943147897720337 seconds\n",
      "Step 763 / 1000 - slice 134874:135051 DONE in 11.895373106002808 seconds\n",
      "Step 764 / 1000 - slice 135051:135228 DONE in 10.9563729763031 seconds\n",
      "Step 765 / 1000 - slice 135228:135405 DONE in 11.752729892730713 seconds\n",
      "Step 766 / 1000 - slice 135405:135582 DONE in 11.671056270599365 seconds\n",
      "Step 767 / 1000 - slice 135582:135759 DONE in 11.12328290939331 seconds\n",
      "Step 768 / 1000 - slice 135759:135936 DONE in 11.038529872894287 seconds\n",
      "Step 769 / 1000 - slice 135936:136113 DONE in 10.984596252441406 seconds\n",
      "Step 770 / 1000 - slice 136113:136290 DONE in 11.09512996673584 seconds\n",
      "Step 771 / 1000 - slice 136290:136467 DONE in 11.108197927474976 seconds\n",
      "Step 772 / 1000 - slice 136467:136644 DONE in 10.981930017471313 seconds\n",
      "Step 773 / 1000 - slice 136644:136821 DONE in 10.983633995056152 seconds\n",
      "Step 774 / 1000 - slice 136821:136998 DONE in 11.019967317581177 seconds\n",
      "Step 775 / 1000 - slice 136998:137175 DONE in 10.90717887878418 seconds\n",
      "Step 776 / 1000 - slice 137175:137352 DONE in 10.986103057861328 seconds\n",
      "Step 777 / 1000 - slice 137352:137529 DONE in 11.046754837036133 seconds\n",
      "Step 778 / 1000 - slice 137529:137706 DONE in 10.931189775466919 seconds\n",
      "Step 779 / 1000 - slice 137706:137883 DONE in 11.025071859359741 seconds\n",
      "Step 780 / 1000 - slice 137883:138060 DONE in 10.875643730163574 seconds\n",
      "Step 781 / 1000 - slice 138060:138237 DONE in 10.998888969421387 seconds\n",
      "Step 782 / 1000 - slice 138237:138414 DONE in 11.060719966888428 seconds\n",
      "Step 783 / 1000 - slice 138414:138591 DONE in 10.994298696517944 seconds\n",
      "Step 784 / 1000 - slice 138591:138768 DONE in 10.98851203918457 seconds\n",
      "Step 785 / 1000 - slice 138768:138945 DONE in 11.08922791481018 seconds\n",
      "Step 786 / 1000 - slice 138945:139122 DONE in 10.945292711257935 seconds\n",
      "Step 787 / 1000 - slice 139122:139299 DONE in 12.522777080535889 seconds\n",
      "Step 788 / 1000 - slice 139299:139476 DONE in 13.177955865859985 seconds\n",
      "Step 789 / 1000 - slice 139476:139653 DONE in 13.731983184814453 seconds\n",
      "Step 790 / 1000 - slice 139653:139830 DONE in 13.02971601486206 seconds\n",
      "Step 791 / 1000 - slice 139830:140007 DONE in 11.415592908859253 seconds\n",
      "Step 792 / 1000 - slice 140007:140184 DONE in 13.4239821434021 seconds\n",
      "Step 793 / 1000 - slice 140184:140361 DONE in 12.94004511833191 seconds\n",
      "Step 794 / 1000 - slice 140361:140538 DONE in 11.184349060058594 seconds\n",
      "Step 795 / 1000 - slice 140538:140715 DONE in 10.97882890701294 seconds\n",
      "Step 796 / 1000 - slice 140715:140892 DONE in 10.866011142730713 seconds\n",
      "Step 797 / 1000 - slice 140892:141069 DONE in 11.21754503250122 seconds\n",
      "Step 798 / 1000 - slice 141069:141246 DONE in 11.639056921005249 seconds\n",
      "Step 799 / 1000 - slice 141246:141423 DONE in 11.564729928970337 seconds\n",
      "Step 800 / 1000 - slice 141423:141600 DONE in 11.680351972579956 seconds\n",
      "Step 801 / 1000 - slice 141600:141777 DONE in 12.172338962554932 seconds\n",
      "Step 802 / 1000 - slice 141777:141954 DONE in 11.48125410079956 seconds\n",
      "Step 803 / 1000 - slice 141954:142131 DONE in 11.460429906845093 seconds\n",
      "Step 804 / 1000 - slice 142131:142308 DONE in 11.067472219467163 seconds\n",
      "Step 805 / 1000 - slice 142308:142485 DONE in 11.181731224060059 seconds\n",
      "Step 806 / 1000 - slice 142485:142662 DONE in 13.248584985733032 seconds\n",
      "Step 807 / 1000 - slice 142662:142839 DONE in 14.001252174377441 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 808 / 1000 - slice 142839:143016 DONE in 13.107497930526733 seconds\n",
      "Step 809 / 1000 - slice 143016:143193 DONE in 12.9666268825531 seconds\n",
      "Step 810 / 1000 - slice 143193:143370 DONE in 12.920257091522217 seconds\n",
      "Step 811 / 1000 - slice 143370:143547 DONE in 13.078426122665405 seconds\n",
      "Step 812 / 1000 - slice 143547:143724 DONE in 12.130358219146729 seconds\n",
      "Step 813 / 1000 - slice 143724:143901 DONE in 12.036061763763428 seconds\n",
      "Step 814 / 1000 - slice 143901:144078 DONE in 13.739752054214478 seconds\n",
      "Step 815 / 1000 - slice 144078:144255 DONE in 12.156924962997437 seconds\n",
      "Step 816 / 1000 - slice 144255:144432 DONE in 13.025922060012817 seconds\n",
      "Step 817 / 1000 - slice 144432:144609 DONE in 12.866425037384033 seconds\n",
      "Step 818 / 1000 - slice 144609:144786 DONE in 14.123568058013916 seconds\n",
      "Step 819 / 1000 - slice 144786:144963 DONE in 12.244639158248901 seconds\n",
      "Step 820 / 1000 - slice 144963:145140 DONE in 11.861634969711304 seconds\n",
      "Step 821 / 1000 - slice 145140:145317 DONE in 11.920764207839966 seconds\n",
      "Step 822 / 1000 - slice 145317:145494 DONE in 11.98375678062439 seconds\n",
      "Step 823 / 1000 - slice 145494:145671 DONE in 11.380605936050415 seconds\n",
      "Step 824 / 1000 - slice 145671:145848 DONE in 11.347561120986938 seconds\n",
      "Step 825 / 1000 - slice 145848:146025 DONE in 11.341062068939209 seconds\n",
      "Step 826 / 1000 - slice 146025:146202 DONE in 11.795799970626831 seconds\n",
      "Step 827 / 1000 - slice 146202:146379 DONE in 12.17867922782898 seconds\n",
      "Step 828 / 1000 - slice 146379:146556 DONE in 12.078584909439087 seconds\n",
      "Step 829 / 1000 - slice 146556:146733 DONE in 12.04130506515503 seconds\n",
      "Step 830 / 1000 - slice 146733:146910 DONE in 11.022336721420288 seconds\n",
      "Step 831 / 1000 - slice 146910:147087 DONE in 10.970411777496338 seconds\n",
      "Step 832 / 1000 - slice 147087:147264 DONE in 11.018792390823364 seconds\n",
      "Step 833 / 1000 - slice 147264:147441 DONE in 12.454272985458374 seconds\n",
      "Step 834 / 1000 - slice 147441:147618 DONE in 11.20389986038208 seconds\n",
      "Step 835 / 1000 - slice 147618:147795 DONE in 11.207200050354004 seconds\n",
      "Step 836 / 1000 - slice 147795:147972 DONE in 11.295325994491577 seconds\n",
      "Step 837 / 1000 - slice 147972:148149 DONE in 11.561131954193115 seconds\n",
      "Step 838 / 1000 - slice 148149:148326 DONE in 12.55305004119873 seconds\n",
      "Step 839 / 1000 - slice 148326:148503 DONE in 12.198857069015503 seconds\n",
      "Step 840 / 1000 - slice 148503:148680 DONE in 11.34869122505188 seconds\n",
      "Step 841 / 1000 - slice 148680:148857 DONE in 10.938873052597046 seconds\n",
      "Step 842 / 1000 - slice 148857:149034 DONE in 11.138390064239502 seconds\n",
      "Step 843 / 1000 - slice 149034:149211 DONE in 11.013931035995483 seconds\n",
      "Step 844 / 1000 - slice 149211:149388 DONE in 11.61919903755188 seconds\n",
      "Step 845 / 1000 - slice 149388:149565 DONE in 10.933588981628418 seconds\n",
      "Step 846 / 1000 - slice 149565:149742 DONE in 11.03390884399414 seconds\n",
      "Step 847 / 1000 - slice 149742:149919 DONE in 10.900333642959595 seconds\n",
      "Step 848 / 1000 - slice 149919:150096 DONE in 11.001056909561157 seconds\n",
      "Step 849 / 1000 - slice 150096:150273 DONE in 11.096419095993042 seconds\n",
      "Step 850 / 1000 - slice 150273:150450 DONE in 11.101462125778198 seconds\n",
      "Step 851 / 1000 - slice 150450:150627 DONE in 10.885426998138428 seconds\n",
      "Step 852 / 1000 - slice 150627:150804 DONE in 11.026803970336914 seconds\n",
      "Step 853 / 1000 - slice 150804:150981 DONE in 11.030155897140503 seconds\n",
      "Step 854 / 1000 - slice 150981:151158 DONE in 11.070203065872192 seconds\n",
      "Step 855 / 1000 - slice 151158:151335 DONE in 10.887234926223755 seconds\n",
      "Step 856 / 1000 - slice 151335:151512 DONE in 11.143132209777832 seconds\n",
      "Step 857 / 1000 - slice 151512:151689 DONE in 11.091070890426636 seconds\n",
      "Step 858 / 1000 - slice 151689:151866 DONE in 10.996996879577637 seconds\n",
      "Step 859 / 1000 - slice 151866:152043 DONE in 11.103204011917114 seconds\n",
      "Step 860 / 1000 - slice 152043:152220 DONE in 11.552817821502686 seconds\n",
      "Step 861 / 1000 - slice 152220:152397 DONE in 10.908354043960571 seconds\n",
      "Step 862 / 1000 - slice 152397:152574 DONE in 10.968086004257202 seconds\n",
      "Step 863 / 1000 - slice 152574:152751 DONE in 10.887577056884766 seconds\n",
      "Step 864 / 1000 - slice 152751:152928 DONE in 10.87985610961914 seconds\n",
      "Step 865 / 1000 - slice 152928:153105 DONE in 10.856178045272827 seconds\n",
      "Step 866 / 1000 - slice 153105:153282 DONE in 10.918567895889282 seconds\n",
      "Step 867 / 1000 - slice 153282:153459 DONE in 10.983531951904297 seconds\n",
      "Step 868 / 1000 - slice 153459:153636 DONE in 10.961199760437012 seconds\n",
      "Step 869 / 1000 - slice 153636:153813 DONE in 10.92836594581604 seconds\n",
      "Step 870 / 1000 - slice 153813:153990 DONE in 14.990952253341675 seconds\n",
      "Step 871 / 1000 - slice 153990:154167 DONE in 12.92022705078125 seconds\n",
      "Step 872 / 1000 - slice 154167:154344 DONE in 13.355557918548584 seconds\n",
      "Step 873 / 1000 - slice 154344:154521 DONE in 12.33323884010315 seconds\n",
      "Step 874 / 1000 - slice 154521:154698 DONE in 11.222430229187012 seconds\n",
      "Step 875 / 1000 - slice 154698:154875 DONE in 11.295914888381958 seconds\n",
      "Step 876 / 1000 - slice 154875:155052 DONE in 10.995307207107544 seconds\n",
      "Step 877 / 1000 - slice 155052:155229 DONE in 11.014097929000854 seconds\n",
      "Step 878 / 1000 - slice 155229:155406 DONE in 12.063281774520874 seconds\n",
      "Step 879 / 1000 - slice 155406:155583 DONE in 12.005125999450684 seconds\n",
      "Step 880 / 1000 - slice 155583:155760 DONE in 11.24518871307373 seconds\n",
      "Step 881 / 1000 - slice 155760:155937 DONE in 12.209600925445557 seconds\n",
      "Step 882 / 1000 - slice 155937:156114 DONE in 19.509397983551025 seconds\n",
      "Step 883 / 1000 - slice 156114:156291 DONE in 13.40454387664795 seconds\n",
      "Step 884 / 1000 - slice 156291:156468 DONE in 13.98251986503601 seconds\n",
      "Step 885 / 1000 - slice 156468:156645 DONE in 13.142456769943237 seconds\n",
      "Step 886 / 1000 - slice 156645:156822 DONE in 12.431712865829468 seconds\n",
      "Step 887 / 1000 - slice 156822:156999 DONE in 11.230475902557373 seconds\n",
      "Step 888 / 1000 - slice 156999:157176 DONE in 11.199934959411621 seconds\n",
      "Step 889 / 1000 - slice 157176:157353 DONE in 11.560529947280884 seconds\n",
      "Step 890 / 1000 - slice 157353:157530 DONE in 10.944884777069092 seconds\n",
      "Step 891 / 1000 - slice 157530:157707 DONE in 11.052228927612305 seconds\n",
      "Step 892 / 1000 - slice 157707:157884 DONE in 10.959865093231201 seconds\n",
      "Step 893 / 1000 - slice 157884:158061 DONE in 10.928131103515625 seconds\n",
      "Step 894 / 1000 - slice 158061:158238 DONE in 11.432501077651978 seconds\n",
      "Step 895 / 1000 - slice 158238:158415 DONE in 10.965396165847778 seconds\n",
      "Step 896 / 1000 - slice 158415:158592 DONE in 10.869871854782104 seconds\n",
      "Step 897 / 1000 - slice 158592:158769 DONE in 11.64861512184143 seconds\n",
      "Step 898 / 1000 - slice 158769:158946 DONE in 10.904798984527588 seconds\n",
      "Step 899 / 1000 - slice 158946:159123 DONE in 10.939271926879883 seconds\n",
      "Step 900 / 1000 - slice 159123:159300 DONE in 10.941780090332031 seconds\n",
      "Step 901 / 1000 - slice 159300:159477 DONE in 10.943614959716797 seconds\n",
      "Step 902 / 1000 - slice 159477:159654 DONE in 11.133496761322021 seconds\n",
      "Step 903 / 1000 - slice 159654:159831 DONE in 10.910315036773682 seconds\n",
      "Step 904 / 1000 - slice 159831:160008 DONE in 11.453328132629395 seconds\n",
      "Step 905 / 1000 - slice 160008:160185 DONE in 10.943633794784546 seconds\n",
      "Step 906 / 1000 - slice 160185:160362 DONE in 10.97105598449707 seconds\n",
      "Step 907 / 1000 - slice 160362:160539 DONE in 11.010967016220093 seconds\n",
      "Step 908 / 1000 - slice 160539:160716 DONE in 11.016095161437988 seconds\n",
      "Step 909 / 1000 - slice 160716:160893 DONE in 11.163586139678955 seconds\n",
      "Step 910 / 1000 - slice 160893:161070 DONE in 10.970306158065796 seconds\n",
      "Step 911 / 1000 - slice 161070:161247 DONE in 10.951446771621704 seconds\n",
      "Step 912 / 1000 - slice 161247:161424 DONE in 10.959526777267456 seconds\n",
      "Step 913 / 1000 - slice 161424:161601 DONE in 11.012517929077148 seconds\n",
      "Step 914 / 1000 - slice 161601:161778 DONE in 11.179667949676514 seconds\n",
      "Step 915 / 1000 - slice 161778:161955 DONE in 11.001269102096558 seconds\n",
      "Step 916 / 1000 - slice 161955:162132 DONE in 11.015832901000977 seconds\n",
      "Step 917 / 1000 - slice 162132:162309 DONE in 10.944890260696411 seconds\n",
      "Step 918 / 1000 - slice 162309:162486 DONE in 10.96727991104126 seconds\n",
      "Step 919 / 1000 - slice 162486:162663 DONE in 11.108110904693604 seconds\n",
      "Step 920 / 1000 - slice 162663:162840 DONE in 10.964107990264893 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 921 / 1000 - slice 162840:163017 DONE in 10.981138944625854 seconds\n",
      "Step 922 / 1000 - slice 163017:163194 DONE in 10.864591121673584 seconds\n",
      "Step 923 / 1000 - slice 163194:163371 DONE in 10.90548300743103 seconds\n",
      "Step 924 / 1000 - slice 163371:163548 DONE in 11.786193132400513 seconds\n",
      "Step 925 / 1000 - slice 163548:163725 DONE in 10.978208065032959 seconds\n",
      "Step 926 / 1000 - slice 163725:163902 DONE in 10.829850196838379 seconds\n",
      "Step 927 / 1000 - slice 163902:164079 DONE in 10.944430828094482 seconds\n",
      "Step 928 / 1000 - slice 164079:164256 DONE in 10.877121686935425 seconds\n",
      "Step 929 / 1000 - slice 164256:164433 DONE in 10.951154947280884 seconds\n",
      "Step 930 / 1000 - slice 164433:164610 DONE in 10.970303297042847 seconds\n",
      "Step 931 / 1000 - slice 164610:164787 DONE in 10.936965942382812 seconds\n",
      "Step 932 / 1000 - slice 164787:164964 DONE in 10.934694290161133 seconds\n",
      "Step 933 / 1000 - slice 164964:165141 DONE in 11.129607915878296 seconds\n",
      "Step 934 / 1000 - slice 165141:165318 DONE in 11.259728193283081 seconds\n",
      "Step 935 / 1000 - slice 165318:165495 DONE in 11.081507205963135 seconds\n",
      "Step 936 / 1000 - slice 165495:165672 DONE in 10.896360874176025 seconds\n",
      "Step 937 / 1000 - slice 165672:165849 DONE in 10.900415897369385 seconds\n",
      "Step 938 / 1000 - slice 165849:166026 DONE in 11.099894046783447 seconds\n",
      "Step 939 / 1000 - slice 166026:166203 DONE in 10.92814826965332 seconds\n",
      "Step 940 / 1000 - slice 166203:166380 DONE in 10.920233964920044 seconds\n",
      "Step 941 / 1000 - slice 166380:166557 DONE in 11.535799026489258 seconds\n",
      "Step 942 / 1000 - slice 166557:166734 DONE in 12.388190984725952 seconds\n",
      "Step 943 / 1000 - slice 166734:166911 DONE in 12.318478107452393 seconds\n",
      "Step 944 / 1000 - slice 166911:167088 DONE in 12.630746126174927 seconds\n",
      "Step 945 / 1000 - slice 167088:167265 DONE in 12.351566076278687 seconds\n",
      "Step 946 / 1000 - slice 167265:167442 DONE in 12.240622758865356 seconds\n",
      "Step 947 / 1000 - slice 167442:167619 DONE in 12.390560865402222 seconds\n",
      "Step 948 / 1000 - slice 167619:167796 DONE in 12.271193265914917 seconds\n",
      "Step 949 / 1000 - slice 167796:167973 DONE in 12.37314510345459 seconds\n",
      "Step 950 / 1000 - slice 167973:168150 DONE in 12.377748012542725 seconds\n",
      "Step 951 / 1000 - slice 168150:168327 DONE in 12.883334159851074 seconds\n",
      "Step 952 / 1000 - slice 168327:168504 DONE in 12.32814884185791 seconds\n",
      "Step 953 / 1000 - slice 168504:168681 DONE in 12.708798885345459 seconds\n",
      "Step 954 / 1000 - slice 168681:168858 DONE in 12.419591188430786 seconds\n",
      "Step 955 / 1000 - slice 168858:169035 DONE in 12.301872968673706 seconds\n",
      "Step 956 / 1000 - slice 169035:169212 DONE in 12.354435205459595 seconds\n",
      "Step 957 / 1000 - slice 169212:169389 DONE in 12.416950941085815 seconds\n",
      "Step 958 / 1000 - slice 169389:169566 DONE in 12.408566236495972 seconds\n",
      "Step 959 / 1000 - slice 169566:169743 DONE in 12.292242765426636 seconds\n",
      "Step 960 / 1000 - slice 169743:169920 DONE in 12.279495000839233 seconds\n",
      "Step 961 / 1000 - slice 169920:170097 DONE in 12.415982723236084 seconds\n",
      "Step 962 / 1000 - slice 170097:170274 DONE in 12.332417011260986 seconds\n",
      "Step 963 / 1000 - slice 170274:170451 DONE in 12.276848793029785 seconds\n",
      "Step 964 / 1000 - slice 170451:170628 DONE in 12.35771107673645 seconds\n",
      "Step 965 / 1000 - slice 170628:170805 DONE in 12.68980884552002 seconds\n",
      "Step 966 / 1000 - slice 170805:170982 DONE in 12.21473503112793 seconds\n",
      "Step 967 / 1000 - slice 170982:171159 DONE in 12.431655883789062 seconds\n",
      "Step 968 / 1000 - slice 171159:171336 DONE in 12.337477922439575 seconds\n",
      "Step 969 / 1000 - slice 171336:171513 DONE in 12.33720088005066 seconds\n",
      "Step 970 / 1000 - slice 171513:171690 DONE in 12.293672800064087 seconds\n",
      "Step 971 / 1000 - slice 171690:171867 DONE in 12.689121007919312 seconds\n",
      "Step 972 / 1000 - slice 171867:172044 DONE in 11.711338996887207 seconds\n",
      "Step 973 / 1000 - slice 172044:172221 DONE in 12.392210960388184 seconds\n",
      "Step 974 / 1000 - slice 172221:172398 DONE in 12.523849964141846 seconds\n",
      "Step 975 / 1000 - slice 172398:172575 DONE in 10.923290014266968 seconds\n",
      "Step 976 / 1000 - slice 172575:172752 DONE in 11.420405864715576 seconds\n",
      "Step 977 / 1000 - slice 172752:172929 DONE in 10.92893385887146 seconds\n",
      "Step 978 / 1000 - slice 172929:173106 DONE in 11.191682815551758 seconds\n",
      "Step 979 / 1000 - slice 173106:173283 DONE in 11.04268479347229 seconds\n",
      "Step 980 / 1000 - slice 173283:173460 DONE in 11.363999128341675 seconds\n",
      "Step 981 / 1000 - slice 173460:173637 DONE in 11.491696119308472 seconds\n",
      "Step 982 / 1000 - slice 173637:173814 DONE in 10.969573736190796 seconds\n",
      "Step 983 / 1000 - slice 173814:173991 DONE in 13.70788311958313 seconds\n",
      "Step 984 / 1000 - slice 173991:174168 DONE in 17.05706024169922 seconds\n",
      "Step 985 / 1000 - slice 174168:174345 DONE in 11.854154348373413 seconds\n",
      "Step 986 / 1000 - slice 174345:174522 DONE in 10.970978021621704 seconds\n",
      "Step 987 / 1000 - slice 174522:174699 DONE in 10.927191972732544 seconds\n",
      "Step 988 / 1000 - slice 174699:174876 DONE in 10.91838002204895 seconds\n",
      "Step 989 / 1000 - slice 174876:175053 DONE in 10.952879905700684 seconds\n",
      "Step 990 / 1000 - slice 175053:175230 DONE in 11.035480737686157 seconds\n",
      "Step 991 / 1000 - slice 175230:175407 DONE in 10.879045724868774 seconds\n",
      "Step 992 / 1000 - slice 175407:175584 DONE in 10.987941265106201 seconds\n",
      "Step 993 / 1000 - slice 175584:175761 DONE in 11.290216207504272 seconds\n",
      "Step 994 / 1000 - slice 175761:175938 DONE in 10.86395525932312 seconds\n",
      "Step 995 / 1000 - slice 175938:176115 DONE in 10.889465093612671 seconds\n",
      "Step 996 / 1000 - slice 176115:176292 DONE in 11.036597967147827 seconds\n",
      "Step 997 / 1000 - slice 176292:176469 DONE in 10.897356986999512 seconds\n",
      "Step 998 / 1000 - slice 176469:176646 DONE in 10.963822841644287 seconds\n",
      "Step 999 / 1000 - slice 176646:176823 DONE in 11.105684995651245 seconds\n",
      "Step 1000 / 1000 - slice 176823:177000 DONE in 11.569112062454224 seconds\n",
      "The validation dataset will be created\n",
      "Created dataset validation (100 repetitions)\n",
      "Step 1 / 100 - slice 0:177 DONE in 11.102025985717773 seconds\n",
      "Step 2 / 100 - slice 177:354 DONE in 11.386949062347412 seconds\n",
      "Step 3 / 100 - slice 354:531 DONE in 11.576092958450317 seconds\n",
      "Step 4 / 100 - slice 531:708 DONE in 10.823190212249756 seconds\n",
      "Step 5 / 100 - slice 708:885 DONE in 10.93979287147522 seconds\n",
      "Step 6 / 100 - slice 885:1062 DONE in 11.569831848144531 seconds\n",
      "Step 7 / 100 - slice 1062:1239 DONE in 13.189818143844604 seconds\n",
      "Step 8 / 100 - slice 1239:1416 DONE in 11.212812900543213 seconds\n",
      "Step 9 / 100 - slice 1416:1593 DONE in 15.234608173370361 seconds\n",
      "Step 10 / 100 - slice 1593:1770 DONE in 16.38263177871704 seconds\n",
      "Step 11 / 100 - slice 1770:1947 DONE in 14.584404945373535 seconds\n",
      "Step 12 / 100 - slice 1947:2124 DONE in 12.670974016189575 seconds\n",
      "Step 13 / 100 - slice 2124:2301 DONE in 12.513545274734497 seconds\n",
      "Step 14 / 100 - slice 2301:2478 DONE in 13.045679092407227 seconds\n",
      "Step 15 / 100 - slice 2478:2655 DONE in 13.455024003982544 seconds\n",
      "Step 16 / 100 - slice 2655:2832 DONE in 12.078460931777954 seconds\n",
      "Step 17 / 100 - slice 2832:3009 DONE in 11.833441019058228 seconds\n",
      "Step 18 / 100 - slice 3009:3186 DONE in 11.68721604347229 seconds\n",
      "Step 19 / 100 - slice 3186:3363 DONE in 12.140806913375854 seconds\n",
      "Step 20 / 100 - slice 3363:3540 DONE in 12.044481992721558 seconds\n",
      "Step 21 / 100 - slice 3540:3717 DONE in 12.31477403640747 seconds\n",
      "Step 22 / 100 - slice 3717:3894 DONE in 12.224778890609741 seconds\n",
      "Step 23 / 100 - slice 3894:4071 DONE in 12.166975259780884 seconds\n",
      "Step 24 / 100 - slice 4071:4248 DONE in 12.337887048721313 seconds\n",
      "Step 25 / 100 - slice 4248:4425 DONE in 12.647338628768921 seconds\n",
      "Step 26 / 100 - slice 4425:4602 DONE in 11.866603136062622 seconds\n",
      "Step 27 / 100 - slice 4602:4779 DONE in 11.879320859909058 seconds\n",
      "Step 28 / 100 - slice 4779:4956 DONE in 13.030786991119385 seconds\n",
      "Step 29 / 100 - slice 4956:5133 DONE in 11.9517240524292 seconds\n",
      "Step 30 / 100 - slice 5133:5310 DONE in 11.809351921081543 seconds\n",
      "Step 31 / 100 - slice 5310:5487 DONE in 12.447230815887451 seconds\n",
      "Step 32 / 100 - slice 5487:5664 DONE in 12.610638856887817 seconds\n",
      "Step 33 / 100 - slice 5664:5841 DONE in 12.11191987991333 seconds\n",
      "Step 34 / 100 - slice 5841:6018 DONE in 11.941001892089844 seconds\n",
      "Step 35 / 100 - slice 6018:6195 DONE in 11.933526039123535 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 36 / 100 - slice 6195:6372 DONE in 11.866087913513184 seconds\n",
      "Step 37 / 100 - slice 6372:6549 DONE in 11.907933235168457 seconds\n",
      "Step 38 / 100 - slice 6549:6726 DONE in 11.898898124694824 seconds\n",
      "Step 39 / 100 - slice 6726:6903 DONE in 11.97248888015747 seconds\n",
      "Step 40 / 100 - slice 6903:7080 DONE in 12.029602766036987 seconds\n",
      "Step 41 / 100 - slice 7080:7257 DONE in 12.439888954162598 seconds\n",
      "Step 42 / 100 - slice 7257:7434 DONE in 12.027023792266846 seconds\n",
      "Step 43 / 100 - slice 7434:7611 DONE in 11.967891931533813 seconds\n",
      "Step 44 / 100 - slice 7611:7788 DONE in 11.990520000457764 seconds\n",
      "Step 45 / 100 - slice 7788:7965 DONE in 11.86212682723999 seconds\n",
      "Step 46 / 100 - slice 7965:8142 DONE in 11.871284008026123 seconds\n",
      "Step 47 / 100 - slice 8142:8319 DONE in 11.853605031967163 seconds\n",
      "Step 48 / 100 - slice 8319:8496 DONE in 11.89610505104065 seconds\n",
      "Step 49 / 100 - slice 8496:8673 DONE in 11.834949016571045 seconds\n",
      "Step 50 / 100 - slice 8673:8850 DONE in 12.103639125823975 seconds\n",
      "Step 51 / 100 - slice 8850:9027 DONE in 12.106462001800537 seconds\n",
      "Step 52 / 100 - slice 9027:9204 DONE in 12.053713083267212 seconds\n",
      "Step 53 / 100 - slice 9204:9381 DONE in 11.923211097717285 seconds\n",
      "Step 54 / 100 - slice 9381:9558 DONE in 11.914242029190063 seconds\n",
      "Step 55 / 100 - slice 9558:9735 DONE in 12.696707248687744 seconds\n",
      "Step 56 / 100 - slice 9735:9912 DONE in 12.05333399772644 seconds\n",
      "Step 57 / 100 - slice 9912:10089 DONE in 11.63112998008728 seconds\n",
      "Step 58 / 100 - slice 10089:10266 DONE in 12.025605916976929 seconds\n",
      "Step 59 / 100 - slice 10266:10443 DONE in 11.977203130722046 seconds\n",
      "Step 60 / 100 - slice 10443:10620 DONE in 12.40195894241333 seconds\n",
      "Step 61 / 100 - slice 10620:10797 DONE in 12.044667959213257 seconds\n",
      "Step 62 / 100 - slice 10797:10974 DONE in 12.11194896697998 seconds\n",
      "Step 63 / 100 - slice 10974:11151 DONE in 11.95150113105774 seconds\n",
      "Step 64 / 100 - slice 11151:11328 DONE in 13.661878108978271 seconds\n",
      "Step 65 / 100 - slice 11328:11505 DONE in 13.760004997253418 seconds\n",
      "Step 66 / 100 - slice 11505:11682 DONE in 13.563982009887695 seconds\n",
      "Step 67 / 100 - slice 11682:11859 DONE in 13.960694074630737 seconds\n",
      "Step 68 / 100 - slice 11859:12036 DONE in 13.795024871826172 seconds\n",
      "Step 69 / 100 - slice 12036:12213 DONE in 13.711575746536255 seconds\n",
      "Step 70 / 100 - slice 12213:12390 DONE in 14.34943175315857 seconds\n",
      "Step 71 / 100 - slice 12390:12567 DONE in 13.870629072189331 seconds\n",
      "Step 72 / 100 - slice 12567:12744 DONE in 13.629929065704346 seconds\n",
      "Step 73 / 100 - slice 12744:12921 DONE in 13.654173851013184 seconds\n",
      "Step 74 / 100 - slice 12921:13098 DONE in 13.74688196182251 seconds\n",
      "Step 75 / 100 - slice 13098:13275 DONE in 13.60892105102539 seconds\n",
      "Step 76 / 100 - slice 13275:13452 DONE in 13.813190937042236 seconds\n",
      "Step 77 / 100 - slice 13452:13629 DONE in 13.688915014266968 seconds\n",
      "Step 78 / 100 - slice 13629:13806 DONE in 13.791937112808228 seconds\n",
      "Step 79 / 100 - slice 13806:13983 DONE in 14.482740879058838 seconds\n",
      "Step 80 / 100 - slice 13983:14160 DONE in 13.651035070419312 seconds\n",
      "Step 81 / 100 - slice 14160:14337 DONE in 14.30725622177124 seconds\n",
      "Step 82 / 100 - slice 14337:14514 DONE in 13.685689687728882 seconds\n",
      "Step 83 / 100 - slice 14514:14691 DONE in 12.447910785675049 seconds\n",
      "Step 84 / 100 - slice 14691:14868 DONE in 12.413443803787231 seconds\n",
      "Step 85 / 100 - slice 14868:15045 DONE in 12.159709215164185 seconds\n",
      "Step 86 / 100 - slice 15045:15222 DONE in 12.182847023010254 seconds\n",
      "Step 87 / 100 - slice 15222:15399 DONE in 12.213874816894531 seconds\n",
      "Step 88 / 100 - slice 15399:15576 DONE in 12.232623100280762 seconds\n",
      "Step 89 / 100 - slice 15576:15753 DONE in 12.413869142532349 seconds\n",
      "Step 90 / 100 - slice 15753:15930 DONE in 12.126917839050293 seconds\n",
      "Step 91 / 100 - slice 15930:16107 DONE in 12.322131872177124 seconds\n",
      "Step 92 / 100 - slice 16107:16284 DONE in 12.281796932220459 seconds\n",
      "Step 93 / 100 - slice 16284:16461 DONE in 12.70004677772522 seconds\n",
      "Step 94 / 100 - slice 16461:16638 DONE in 12.40688705444336 seconds\n",
      "Step 95 / 100 - slice 16638:16815 DONE in 12.244805097579956 seconds\n",
      "Step 96 / 100 - slice 16815:16992 DONE in 12.410281896591187 seconds\n",
      "Step 97 / 100 - slice 16992:17169 DONE in 12.272311925888062 seconds\n",
      "Step 98 / 100 - slice 17169:17346 DONE in 12.281084775924683 seconds\n",
      "Step 99 / 100 - slice 17346:17523 DONE in 12.280819177627563 seconds\n",
      "Step 100 / 100 - slice 17523:17700 DONE in 12.243265151977539 seconds\n"
     ]
    }
   ],
   "source": [
    "# Read and augment data\n",
    "if architecture == 'resnet':\n",
    "  target_size = (200,200)\n",
    "  K.set_image_data_format('channels_last')\n",
    "elif architecture == 'alexnet':\n",
    "  target_size = (227,227)\n",
    "  K.set_image_data_format('channels_first')\n",
    "else:\n",
    "  raise ValueError(\"Architecture type not available\")\n",
    "def preproc_fn(x):\n",
    "  x -= np.mean(x, keepdims=True)\n",
    "  x /= (np.std(x, keepdims=True) + 1E-7)\n",
    "  x = .5 * (np.tanh(.5 * x) + 1) # this is a sigmoid\n",
    "  return x\n",
    "datagen = H5DataGen(\n",
    "  width_shift_range=0.2,\n",
    "  height_shift_range=0.2,\n",
    "  rotation_range = 50, # degrees (int)\n",
    "  shear_range = 20.*np.pi/180., # radians (float)\n",
    "  zoom_range = 0.1,\n",
    "  fill_mode = 'constant',\n",
    "  cval = 0,\n",
    "  horizontal_flip = True,\n",
    "  preprocessing_function = preproc_fn)\n",
    "idg_args = {\n",
    "  'target_size': target_size,\n",
    "  'color_mode':'rgb',\n",
    "  'class_mode': None,\n",
    "  'batch_size': batch_size,\n",
    "  'shuffle': True\n",
    "}\n",
    "data_provider = datagen.flow_from_h5file(db_path, **idg_args)\n",
    "num_classes = data_provider.num_classes\n",
    "printmsg(\"Found\", data_provider.tot_samples, \"images belonging to\", data_provider.num_classes, \"classes.\")\n",
    "# Build network\n",
    "if architecture == 'resnet':\n",
    "  model = ResNet50(include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(target_size[0],target_size[1],3),\n",
    "    pooling='avg')\n",
    "elif architecture == 'alexnet':\n",
    "  model = AlexNet(include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=(3,target_size[0],target_size[1]),\n",
    "        trainable=False)\n",
    "else:\n",
    "  raise ValueError(\"Architecture type not available\")\n",
    "model.compile(optimizer='rmsprop', loss='mse')\n",
    "n_logits = model.output_shape[1]\n",
    "printmsg(\"Network created -\", n_logits, \"logits\")\n",
    "# Start writing the databases\n",
    "read_mode = 'a'\n",
    "try:\n",
    "  with h5py.File(filename, read_mode) as f:\n",
    "    printmsg(\"Output file\", filename, \"opened\")\n",
    "    def db_append(name, rep):\n",
    "      if not name in f.keys():\n",
    "        printmsg(\"The\", name, \"dataset will be created\")\n",
    "        db = f.create_dataset(name=name, \n",
    "          shape=(rep*num_classes, n_logits), dtype='f',\n",
    "          maxshape=(None, n_logits),\n",
    "          compression=\"gzip\",\n",
    "          compression_opts=9)\n",
    "        db.attrs['num_classes'] = num_classes\n",
    "        db.attrs['repetitions'] = rep\n",
    "        prev_rep = 0\n",
    "        printmsg(\"Created dataset\", name, \"(\"+str(rep)+\" repetitions)\")\n",
    "      else:\n",
    "        db = f[name]\n",
    "        if (not 'num_classes' in db.attrs) or db.attrs['num_classes'] != num_classes:\n",
    "          raise ValueError(\"The dataset lacks 'num_classes' or it differs from input folder traits\")\n",
    "        if (not 'repetitions' in db.attrs):\n",
    "          raise ValueError(\"The dataset lacks 'repetitions'\")\n",
    "        prev_rep = db.attrs['repetitions']\n",
    "        db.attrs['repetitions'] += rep\n",
    "        db.resize((db.shape[0]+rep*num_classes, db.shape[1]))\n",
    "        printmsg(\"Appending logits for\", rep, \"repetitions of\", num_classes, \n",
    "                 \"classes to dataset\", name, \"(total \"+str(db.attrs['repetitions'])+\" repetitions)\")\n",
    "\n",
    "      for k in range(prev_rep, db.attrs['repetitions']):        \n",
    "        begin = k * num_classes # inclusive\n",
    "        end = (k+1) * num_classes # exclusive\n",
    "        time_start = time()\n",
    "        prediction = model.predict_generator(data_provider)\n",
    "        time_end = time()\n",
    "        db[begin:end, :] = prediction\n",
    "        printmsg(\"Step\", k+1, \"/\", db.attrs['repetitions'], \"- slice\", str(begin)+':'+str(end),\n",
    "                 \"DONE in {} seconds\".format(time_end-time_start))\n",
    "\n",
    "    db_append('training', N)\n",
    "    db_append('validation', V)\n",
    "except (Exception, KeyboardInterrupt) as error:\n",
    "  print(\"An error occurred!!\", str(error))\n",
    "  to_be_removed = bool(input('The database is no more valid, shall I remove it? [y]/n')!='n')\n",
    "  if to_be_removed:\n",
    "    os.remove(filename)\n",
    "    print(\"Database {} removed\".format(filename))\n",
    "  else:\n",
    "    print(\"Database {} not removed\".format(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FjdtvDONJhdW"
   },
   "source": [
    "## Train the last layer of the Net\n",
    "Use the logits computed before to train the last layer previosly removed from the net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3-_4vykvYcra"
   },
   "source": [
    "### Define a folder scanner\n",
    "Define a function that recursively scan the given folder, looking for files with the provided ending string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "CmatmNnkYdMN"
   },
   "outputs": [],
   "source": [
    "def scan_dir(path, ending):\n",
    "\t\"\"\"Recursively scan the folder\"\"\"\n",
    "\tfile_list = []\n",
    "\tdir_list = []\n",
    "\tfor curr_dir, _, local_files in os.walk(path):\n",
    "\t\t# filter local files\n",
    "\t\tlocal_files = [os.path.join(curr_dir, x) for x in local_files if x.endswith(ending)]\n",
    "\t\t# append to global list\n",
    "\t\tfile_list += local_files\n",
    "\t\tif local_files:\n",
    "\t\t\tdir_list.append(curr_dir)\n",
    "\treturn dir_list, file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lkBq4pAsJuqQ"
   },
   "source": [
    "### Define the loss function\n",
    "This is a mix between a softmax and a sigmoid, to handle this peculiar problem where the output is always multiclass, but during training we expect only one predicted class at a time, while at runtime it could also predict multiple classes together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Y0yEm0VbBoFC"
   },
   "outputs": [],
   "source": [
    "def binary_sparse_softmax_cross_entropy(target, output, from_logits=False):\n",
    "\t\"\"\"\n",
    "\tExpects the output of a sigmoid layer, but computes the\n",
    "\tsparse softmax cross entropy.\n",
    "\t\"\"\"\n",
    "\t# TF expects logits, Keras expects probabilities.\n",
    "\tif not from_logits:\n",
    "\t\t# transform from sigmoid back to logits\n",
    "\t\t_epsilon = tf.convert_to_tensor(1E-7, output.dtype.base_dtype)\n",
    "\t\toutput = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n",
    "\t\toutput = tf.log(output / (1 - output))\n",
    "\n",
    "\toutput_shape = output.get_shape()\n",
    "\ttargets = tf.cast(tf.reshape(target, [-1]), 'int64')\n",
    "\tlogits = tf.reshape(output, [-1, int(output_shape[-1])])\n",
    "\tres = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "\t\tlabels=targets,\n",
    "\t\tlogits=logits)\n",
    "\tif len(output_shape) >= 3:\n",
    "\t\t# if our output includes timestep dimension\n",
    "\t\t# or spatial dimensions we need to reshape\n",
    "\t\treturn tf.reshape(res, tf.shape(output)[:-1])\n",
    "\telse:\n",
    "\t\treturn res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "muRAxzwEMk2V"
   },
   "source": [
    "### Define a Tensorboard callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ajPxOQkmMkHE"
   },
   "outputs": [],
   "source": [
    "class TensorboardCallback(Callback):\n",
    "\tdef __init__(self, path, args=None, events_dir=None, max_step=None, save_period=10):\n",
    "\t\tself.save_period = save_period\n",
    "\t\tself.path = path\n",
    "\t\ttrain_dir = os.path.join(path, 'training')\n",
    "\t\tif not os.path.exists(train_dir): os.makedirs(train_dir)\n",
    "\t\tself.train_logger = Logger(train_dir)\n",
    "\t\tvalid_dir = os.path.join(path, 'validation')\n",
    "\t\tif not os.path.exists(valid_dir): os.makedirs(valid_dir)\n",
    "\t\tself.valid_logger = Logger(valid_dir)\n",
    "\t\tif args:\n",
    "\t\t\ttext = 'Parameters\\n---------\\n'\n",
    "\t\t\tfor (key, val) in args.items():\n",
    "\t\t\t\ttext += '- '+key+' = '+str(val)+'\\n'\n",
    "\t\t\tself.train_logger.log_text('Description', text)\n",
    "\t\t\tself.valid_logger.log_text('Description', text)\n",
    "\t\tif events_dir and max_step:\n",
    "\t\t\tevents_files = [F for F in scan_dir(events_dir, '')[1] if os.path.basename(F).startswith('events')]\n",
    "\t\t\tfor events_file in events_files:\n",
    "\t\t\t\tparent_dir = os.path.dirname(events_file).split(os.sep)[-1]\n",
    "\t\t\t\tif 'training' == parent_dir:\n",
    "\t\t\t\t\ttrain_events_file = events_file\n",
    "\t\t\t\telif 'validation' == parent_dir:\n",
    "\t\t\t\t\tvalid_events_file = events_file\n",
    "\t\t\tself.train_logger.copyFrom(train_events_file, max_step=max_step)\n",
    "\t\t\tself.valid_logger.copyFrom(valid_events_file, max_step=max_step)\n",
    "\tdef on_epoch_begin(self, epoch, logs={}):\n",
    "\t\tself.starttime=time()\n",
    "\tdef on_epoch_end(self, epoch, logs={}):\n",
    "\t\tself.train_logger.log_scalar(\"Speed\", time()-self.starttime, epoch)\n",
    "\t\tself.train_logger.log_scalar(\"sparse_categorical_accuracy_%\", logs['sparse_categorical_accuracy']*100, epoch)\n",
    "\t\tself.train_logger.log_scalar(\"loss\", logs['loss'], epoch)\n",
    "\t\tself.valid_logger.log_scalar(\"Speed\", time()-self.starttime, epoch)\n",
    "\t\tself.valid_logger.log_scalar(\"sparse_categorical_accuracy_%\", logs['val_sparse_categorical_accuracy']*100, epoch)\n",
    "\t\tself.valid_logger.log_scalar(\"loss\", logs['val_loss'], epoch)\n",
    "\t\t# Model save\n",
    "\t\tif ((epoch+1) % self.save_period) == 0:\n",
    "\t\t\tself.model.save(os.path.join(self.path, 'save_'+str(epoch)+'.h5'))\n",
    "\t\t\t_, oldsaves = scan_dir(self.path, '.h5')\n",
    "\t\t\tfor save in oldsaves:\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tif int(save.split('.')[-2].split('_')[-1]) < epoch:\n",
    "\t\t\t\t\t\tos.remove(save)\n",
    "\t\t\t\texcept: continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t9CK2akMP9f9"
   },
   "source": [
    "### Assign parameters\n",
    "From ``` batch_size ``` on it is possible to assign comma separated values to parameters. In case of multiple values every possible combination is used for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "form",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1012,
     "status": "error",
     "timestamp": 1528531266886,
     "user": {
      "displayName": "FILIPPO SANTARELLI",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "106244644809659945153"
     },
     "user_tz": -120
    },
    "id": "zIejlWpmS-BL",
    "outputId": "48b599ad-2bce-4142-c3ec-a5a425d7e237"
   },
   "outputs": [],
   "source": [
    "#@{ run: \"auto\", display-mode: \"form\" }\n",
    "nb_epoch = 50 #@param {type:\"integer\"}\n",
    "save_epochs = 5 #@param {type:\"integer\"}\n",
    "db_path = \"/Users/MacD/Databases/logits.h5\" #@param {type:\"string\"}\n",
    "load_path = \"\" #@param {type:\"string\"}\n",
    "output_folder = \"/Users/MacD/Databases/training_log\" #@param {type:\"string\"}\n",
    "batch_size = \"256\" #@param {type:\"string\"}\n",
    "learning_rate = \"1e-6\" #@param {type:\"string\"}\n",
    "decay_rate = \"None\" #@param {type:\"string\"}\n",
    "ES_patience = \"None\" #@param {type:\"string\"}\n",
    "ES_mindelta = \"None\" #@param {type:\"string\"}\n",
    "RLROP_patience = \"None\" #@param {type:\"string\"}\n",
    "RLROP_factor = \"None\" #@param {type:\"string\"}\n",
    "db_path = os.path.abspath(os.path.normpath(db_path))\n",
    "def tuplify(func, str_):\n",
    "  if str_ == 'None':\n",
    "    return [None]\n",
    "  else:\n",
    "    return [func(x) for x in str_.split('/')]\n",
    "batch_size = tuplify(int, batch_size)\n",
    "learning_rate = tuplify(float, learning_rate)\n",
    "decay_rate = tuplify(float, decay_rate)\n",
    "ES_patience = tuplify(int, ES_patience)\n",
    "ES_mindelta = tuplify(float, ES_mindelta)\n",
    "RLROP_patience = tuplify(int, RLROP_patience)\n",
    "RLROP_factor = tuplify(float, RLROP_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u01QjvL-Tz58"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1823
    },
    "colab_type": "code",
    "id": "h1iNgqNWS4Y8",
    "outputId": "c9e6c33c-78fb-45fc-b923-87eb43293e27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validation data loaded\n",
      "Training data: 177 classes repeated 2000 times\n",
      "Validation data: 177 classes repeated 200 times\n",
      "(354000, 4096) (354000,)\n",
      "(35400, 4096) (35400,)\n",
      "1 total tests will be performed... be patient!\n",
      "Created log folder: /Users/MacD/Databases/training_log/2018_06_26_21_44_50\n",
      "Train on 354000 samples, validate on 35400 samples\n",
      "Monitor initialized.\n",
      "Name of the model is \"A Keras model\"\n",
      "Model ID is 1153cb1f-27af-40ae-89fe-6337711498a2\n",
      "Training ID is 9597044e-ac80-4166-86a8-b440621dd20a\n",
      "Training started at 2018-06-26 21:44 for 50 epochs with 354000 samples with a 2 layers model.\n",
      "Epoch 1/50\n",
      "354000/354000 [==============================] - 335s 947us/step - loss: 5.1789 - sparse_categorical_accuracy: 0.0056 - val_loss: 5.1771 - val_sparse_categorical_accuracy: 0.0058\n",
      "Epoch 1/50 is done at 2018-06-26 21:50. Average minutes/epoch is 5.59.\n",
      "Logs are : val_loss = 5.177099 | val_sparse_categorical_accuracy = 0.005763 | loss = 5.178904 | sparse_categorical_accuracy = 0.005616\n",
      "Epoch 2/50\n",
      "354000/354000 [==============================] - 332s 938us/step - loss: 5.1766 - sparse_categorical_accuracy: 0.0057 - val_loss: 5.1764 - val_sparse_categorical_accuracy: 0.0056\n",
      "Epoch 2/50 is done at 2018-06-26 21:55. Average minutes/epoch is 5.56.\n",
      "Logs are : val_loss = 5.176400 | val_sparse_categorical_accuracy = 0.005650 | loss = 5.176562 | sparse_categorical_accuracy = 0.005658\n",
      "Epoch 3/50\n",
      "354000/354000 [==============================] - 326s 920us/step - loss: 5.1763 - sparse_categorical_accuracy: 0.0057 - val_loss: 5.1763 - val_sparse_categorical_accuracy: 0.0054\n",
      "Epoch 3/50 is done at 2018-06-26 22:01. Average minutes/epoch is 5.52.\n",
      "Logs are : val_loss = 5.176318 | val_sparse_categorical_accuracy = 0.005395 | loss = 5.176258 | sparse_categorical_accuracy = 0.005743\n",
      "Epoch 4/50\n",
      "354000/354000 [==============================] - 313s 885us/step - loss: 5.1762 - sparse_categorical_accuracy: 0.0058 - val_loss: 5.1763 - val_sparse_categorical_accuracy: 0.0054\n",
      "Epoch 4/50 is done at 2018-06-26 22:06. Average minutes/epoch is 5.44.\n",
      "Logs are : val_loss = 5.176308 | val_sparse_categorical_accuracy = 0.005424 | loss = 5.176209 | sparse_categorical_accuracy = 0.005754\n",
      "Epoch 5/50\n",
      "354000/354000 [==============================] - 313s 885us/step - loss: 5.1762 - sparse_categorical_accuracy: 0.0057 - val_loss: 5.1763 - val_sparse_categorical_accuracy: 0.0052\n",
      "Epoch 5/50 is done at 2018-06-26 22:11. Average minutes/epoch is 5.40.\n",
      "Logs are : val_loss = 5.176305 | val_sparse_categorical_accuracy = 0.005226 | loss = 5.176188 | sparse_categorical_accuracy = 0.005737\n",
      "Epoch 6/50\n",
      "354000/354000 [==============================] - 326s 920us/step - loss: 5.1762 - sparse_categorical_accuracy: 0.0059 - val_loss: 5.1763 - val_sparse_categorical_accuracy: 0.0054\n",
      "Epoch 6/50 is done at 2018-06-26 22:17. Average minutes/epoch is 5.40.\n",
      "Logs are : val_loss = 5.176304 | val_sparse_categorical_accuracy = 0.005424 | loss = 5.176170 | sparse_categorical_accuracy = 0.005850\n",
      "Epoch 7/50\n",
      "354000/354000 [==============================] - 332s 937us/step - loss: 5.1762 - sparse_categorical_accuracy: 0.0059 - val_loss: 5.1763 - val_sparse_categorical_accuracy: 0.0054\n",
      "Epoch 7/50 is done at 2018-06-26 22:22. Average minutes/epoch is 5.42.\n",
      "Logs are : val_loss = 5.176303 | val_sparse_categorical_accuracy = 0.005395 | loss = 5.176153 | sparse_categorical_accuracy = 0.005884\n",
      "Epoch 8/50\n",
      "354000/354000 [==============================] - 346s 978us/step - loss: 5.1761 - sparse_categorical_accuracy: 0.0059 - val_loss: 5.1763 - val_sparse_categorical_accuracy: 0.0053\n",
      "Epoch 8/50 is done at 2018-06-26 22:28. Average minutes/epoch is 5.47.\n",
      "Logs are : val_loss = 5.176302 | val_sparse_categorical_accuracy = 0.005339 | loss = 5.176135 | sparse_categorical_accuracy = 0.005887\n",
      "Epoch 9/50\n",
      "133376/354000 [==========>...................] - ETA: 3:37 - loss: 5.1761 - sparse_categorical_accuracy: 0.0062"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-1548f6d2d9e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0mRLROP_patience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrlropp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mRLROP_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrlropf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                 additional_info = additional_info)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-1548f6d2d9e6>\u001b[0m in \u001b[0;36mrunOnce\u001b[0;34m(**roargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;31m# Set variable parameters from cmd line args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    185\u001b[0m                             ins[:-1], batch_ids) + [ins[-1]]\n\u001b[1;32m    186\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                     raise TypeError('TypeError while preparing batch. '\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/utils/io_utils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mmspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0mfspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdxpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dxpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;31m# Patch up the output for NumPy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "with h5py.File(db_path, \"r\") as f:\n",
    "  if 'training' in f.keys() and 'validation' in f.keys():\n",
    "    train_db = f['training']\n",
    "    valid_db = f['validation']\n",
    "    if 'num_classes' in train_db.attrs and 'repetitions' in train_db.attrs:\n",
    "      train_N, train_num_classes = train_db.attrs['repetitions'], train_db.attrs['num_classes']\n",
    "    else: raise ValueError(\"The training dataset lacks 'num_classes' and 'repetitions' attributes\")\n",
    "    if 'num_classes' in valid_db.attrs and 'repetitions' in valid_db.attrs:\n",
    "      valid_N, valid_num_classes = valid_db.attrs['repetitions'], valid_db.attrs['num_classes']\n",
    "    else: raise ValueError(\"The validation dataset lacks 'num_classes' and 'repetitions' attributes\")\n",
    "    if train_num_classes != valid_num_classes:\n",
    "      raise ValueError(\"The number of classes in training and validation databases differ\")\n",
    "    num_classes = train_num_classes\n",
    "  else: raise ValueError(\"The input database lacks training and validation datasets\")\n",
    "print(\"Training and validation data loaded\")\n",
    "print(\"Training data:\", num_classes, \"classes repeated\", train_N, \"times\")\n",
    "print(\"Validation data:\", num_classes, \"classes repeated\", valid_N, \"times\")\n",
    "\n",
    "train_data = HDF5Matrix(db_path, 'training')\n",
    "valid_data = HDF5Matrix(db_path, 'validation')\n",
    "train_labels = np.tile(np.arange(num_classes), (train_N,))\n",
    "valid_labels = np.tile(np.arange(num_classes), (valid_N,))\n",
    "print(train_data.shape, train_labels.shape)\n",
    "print(valid_data.shape, valid_labels.shape)\n",
    "\n",
    "if train_data.shape[1] != valid_data.shape[1]:\n",
    "  ValueError(\"Different model used for training and validation, not allowed\")\n",
    "logits_length = train_data.shape[1]\n",
    "# Get info about loaded data\n",
    "additional_info = {\n",
    "  'Logits length': logits_length,\n",
    "  'Number of classes': num_classes,\n",
    "  'DB training repetitions': train_N,\n",
    "  'Training samples': train_data.shape[0],\n",
    "  'DB validation repetitions': valid_N,\n",
    "  'Validation samples': valid_data.shape[0]\n",
    "}\n",
    "\n",
    "# Define the function that will be executed for each parameter\n",
    "def runOnce(**roargs):\n",
    "  roargs.update(roargs[\"additional_info\"])\n",
    "  roargs.pop(\"additional_info\", None)\n",
    "\n",
    "  summary_folder = str(datetime.now().isoformat(sep='_', timespec='seconds')).replace(':', '_').replace('-', '_')\n",
    "  log_dir = os.path.join(os.path.abspath(output_folder), summary_folder)\n",
    "  if not os.path.exists(log_dir): os.makedirs(log_dir)\n",
    "  print('Created log folder:', log_dir)\n",
    "  if load_path:\n",
    "    initial_epoch = int(load_path.split('_')[-1].split('.')[0])\n",
    "    # Model creation\n",
    "    model = load_model(load_path, custom_objects={'binary_sparse_softmax_cross_entropy': binary_sparse_softmax_cross_entropy})\n",
    "  else:\n",
    "    initial_epoch = 0\n",
    "    # Model creation\n",
    "    logits = Input(shape=(logits_length,))\n",
    "    prediction = logits\n",
    "    prediction = Dense(num_classes, activation='sigmoid', kernel_initializer=\"he_normal\")(prediction)\n",
    "    model = Model(inputs=logits, outputs=prediction)\n",
    "    model.compile(optimizer=Adam(lr=roargs[\"learning_rate\"], amsgrad=True), \n",
    "      loss=binary_sparse_softmax_cross_entropy, # mutually exclusive classes, independent per-class distributions\n",
    "      metrics=[\"sparse_categorical_accuracy\"])\n",
    "  # Model description\n",
    "  trainable_count = int(np.sum([K.count_params(p) for p in set(model.trainable_weights)]))\n",
    "  non_trainable_count = int(np.sum([K.count_params(p) for p in set(model.non_trainable_weights)]))\n",
    "  roargs['Total parameters'] = trainable_count+non_trainable_count\n",
    "  roargs['Trainable parameters'] = trainable_count\n",
    "  roargs['Non-trainable parameters'] = non_trainable_count\n",
    "  # Create custom callback\n",
    "  if load_path: tensorboardCallback = TensorboardCallback(log_dir, roargs, os.path.dirname(load_path), initial_epoch, save_period=args[\"save_epochs\"])\n",
    "  else: tensorboardCallback = TensorboardCallback(log_dir, roargs, save_period=save_epochs)\n",
    "  # Save other information about the model\n",
    "  with open(os.path.join(log_dir, 'summary_'+summary_folder+'.txt'), mode='w') as F:\n",
    "    print2F = lambda s: F.write(s+'\\n')\n",
    "    print2F('------')\n",
    "    print2F(\"Parameters:\")\n",
    "    for (key, val) in roargs.items():\n",
    "      print2F(str(key)+' = '+str(val))\n",
    "    print2F('------')\n",
    "    print2F('Logs will be summarized in ' + log_dir)\n",
    "    model.summary(print_fn=print2F)\n",
    "\n",
    "  # List of callbacks\n",
    "  callbacks = [TerminateOnNaN(), tensorboardCallback, PrintMonitor()]\n",
    "               #,TelegramMonitor(api_token=\"546794449:AAGzmfH9Oa6277Vsl2T9hRrGnNHHSpEMsd8\", chat_id=\"41795159\", plot_history=1)]\n",
    "  if roargs[\"decay_rate\"] and roargs[\"decay_rate\"] > 0:\n",
    "    compute_lr = lambda e: roargs[\"learning_rate\"] * 1./(1. + roargs[\"decay_rate\"] * e)\n",
    "    callbacks.append(LearningRateScheduler(compute_lr, verbose=0))\n",
    "  if roargs[\"ES_mindelta\"] and roargs[\"ES_patience\"]:\n",
    "    callbacks.append(EarlyStopping(monitor='val_loss', min_delta=roargs[\"ES_mindelta\"], patience=roargs[\"ES_patience\"], verbose=1, mode='min'))\n",
    "  if roargs[\"RLROP_factor\"] and roargs[\"RLROP_patience\"] and roargs[\"RLROP_factor\"] <  1:\n",
    "    callbacks.append(ReduceLROnPlateau(monitor='val_loss', factor=roargs[\"RLROP_factor\"], patience=roargs[\"RLROP_patience\"], verbose=1, mode='min', epsilon=0.0001, cooldown=0, min_lr=1e-10))\n",
    "\n",
    "  # Training\n",
    "  model.fit(x = train_data, \n",
    "    y = train_labels,\n",
    "    batch_size = roargs[\"batch_size\"],\n",
    "    epochs=nb_epoch,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(valid_data, valid_labels),\n",
    "    shuffle=False,\n",
    "    initial_epoch=initial_epoch)\n",
    "\n",
    "# Set variable parameters from cmd line args\n",
    "safe_len = lambda _x_: len(_x_) if _x_ else 0\n",
    "n_tests = np.prod([safe_len(batch_size), safe_len(learning_rate), safe_len(decay_rate),\n",
    "                   safe_len(ES_patience), safe_len(ES_mindelta), safe_len(RLROP_patience),\n",
    "                   safe_len(RLROP_factor)])\n",
    "print(n_tests, \"total tests will be performed... be patient!\")\n",
    "try:\n",
    "    for bs in batch_size:\n",
    "      for lr in learning_rate:\n",
    "        for dr in decay_rate:\n",
    "          for esp in ES_patience:\n",
    "            for esm in ES_mindelta:\n",
    "              for rlropp in RLROP_patience:\n",
    "                for rlropf in RLROP_factor:\n",
    "                  runOnce(batch_size = bs,\n",
    "                    learning_rate = lr,\n",
    "                    decay_rate = dr,\n",
    "                    ES_patience = esp,\n",
    "                    ES_mindelta =  esm,\n",
    "                    RLROP_patience = rlropp,\n",
    "                    RLROP_factor = rlropf,\n",
    "                    additional_info = additional_info)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"User interruption\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "xWC5C-Wxohxg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "_2_pretrained_partial_output.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
